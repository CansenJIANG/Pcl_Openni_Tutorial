<!DOCTYPE html>
<!-- saved from url=(0105)http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors) -->
<html lang="en" dir="ltr" class="client-js"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="UTF-8"><title>PCL/OpenNI tutorial 4: 3D object recognition (descriptors) - Robótica - ULE</title>
<meta name="generator" content="MediaWiki 1.22.5">
<link rel="shortcut icon" href="http://robotica.unileon.es/mediawiki/skins/common/images/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="http://robotica.unileon.es/mediawiki/opensearch_desc.php" title="Robótica - ULE (en)">
<link rel="EditURI" type="application/rsd+xml" href="http://robotica.unileon.es/mediawiki/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Robótica - ULE Atom feed" href="http://robotica.unileon.es/mediawiki/index.php?title=Special:RecentChanges&feed=atom">
<link rel="stylesheet" href="http://robotica.unileon.es/mediawiki/load.php?debug=false&lang=en&modules=ext.geshi.local%7Cmediawiki.legacy.commonPrint%2Cshared%7Cskins.vector&only=styles&skin=vector&*">
<style>
.mw-collapsible-toggle{float:right} li .mw-collapsible-toggle{float:none} .mw-collapsible-toggle-li{list-style:none}
/* cache key: wikidb:resourceloader:filter:minify-css:7:4250852ed2349a0d4d0fc6509a3e7d4c */
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:none;z-index:1099;padding:0;margin:-1px -1px 0 0} html > body .suggestions{margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0}.suggestions-result{color:black;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left}.suggestions-result-current{background-color:#4C59A6;color:white}.suggestions-special .special-label{color:gray;text-align:left}.suggestions-special .special-query{color:black;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:silver}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:white}.autoellipsis-matched,.highlight{font-weight:bold}
/* cache key: wikidb:resourceloader:filter:minify-css:7:9780324491b653a3780e2d029bdc140c */
.postedit-container{margin:0 auto;position:fixed;top:0;height:0;left:50%;z-index:1000;font-size:13px}.postedit-container:hover{cursor:pointer}.postedit{position:relative;top:0.6em;left:-50%;padding:.6em 3.6em .6em 1.1em;line-height:1.5625em;color:#626465;background-color:#f4f4f4;border:1px solid #dcd9d9;text-shadow:0 0.0625em 0 rgba(255,255,255,0.5);border-radius:5px;-webkit-box-shadow:0 2px 5px 0 #ccc;box-shadow:0 2px 5px 0 #ccc;-webkit-transition:all 0.25s ease-in-out;-moz-transition:all 0.25s ease-in-out;-ms-transition:all 0.25s ease-in-out;-o-transition:all 0.25s ease-in-out;transition:all 0.25s ease-in-out}.skin-monobook .postedit{top:6em !important}.postedit-faded{opacity:0}.postedit-icon{padding-left:41px;  line-height:25px;background-repeat:no-repeat;background-position:8px 50%}.postedit-icon-checkmark{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAMAAAAoLQ9TAAABblBMVEUAAAD///////9PfTf///80aRdTgjn///9Feij///////////9Rfzf///////////9PfjZRgDh1o1xOfTb///////+bwYqLtnj///////9PfTa82K////9WhT6YxIL///9QgDdTgzr////////j7uDl7eLq8efi693k7OH///////9UhjuBr2rp9uRUhjr///9YljVKgir///9WiTlYjT3////9/v57vFlbkT5PjC9dlD/5/fhuq09stUTs9uhxuElctCpfnT1huDFloEZloUZmpENmvDZpvDxpvTxqvjxrvT5rvT9rwTxsqktswD5uwkBvuUdxw0NztFBztU9ztVBzwkp0tlJ1xkd2t1R3uVR4w1F4xk54x014yE15uVZ5v1R5xVB6v1R7yFJ8wVh9xVl9yFR9yVd9ylN+xVh+yFd/x1l/yFeAylmEx1+Ny2uY0Hqe04Wj1Ymv3Ze33qLD47TJ5L3O6cPU7Mrq9eb2+/Q4j37OAAAAQHRSTlMAAQIEBAUFBQwPFB4fJCUoKiosQEhJS01RUlZZXmdydXaChYuSlJSWmJmoq6uur8LExcvM19fg5ejt8fX2+Pr7SljgewAAAKpJREFUGBkFwQNCAwAAAMDLtl3LtrG4rWXbtvX77gAgZ6grFwC0bhwNVgKgdPZx8b0dgLi+s7Wn0VoAqpfOI9+BNADZI7fLrz2pSEwGHZuH+78lSK8ZLkLezF3ooyUG3VPXq2USei9WngeyoG195yBYWDF3E/2pAhl1e9Gr8bGT+bfOFCC2fnvh4X7rcqIAQNNu+HT6sxkAjceTL/2ZAIhv+PorBwBJxfkA//dFHSCBy/UTAAAAAElFTkSuQmCC);background-image:url(http://robotica.unileon.es/mediawiki/resources/mediawiki.action/images/green-checkmark.png?2014-04-10T08:01:40Z)!ie;background-position:left}.postedit-close{position:absolute;padding:0 .8em;right:0;top:0;font-size:1.25em;font-weight:bold;line-height:2.3em;color:black;text-shadow:0 0.0625em 0 white;text-decoration:none;opacity:0.2;filter:alpha(opacity=20)}.postedit-close:hover{color:black;text-decoration:none;opacity:0.4;filter:alpha(opacity=40)}
/* cache key: wikidb:resourceloader:filter:minify-css:7:9b39df22efb31003b8c266f2194113b0 */</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{text-decoration:none;color:black}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:white}
/* cache key: wikidb:resourceloader:filter:minify-css:7:52b1797f70c7e4094dfa4191101944e8 */</style><style>
table.jquery-tablesorter th.headerSort{background-image:url(data:image/gif;base64,R0lGODlhFQAJAIABAAAAAAAAACH/C1hNUCBEYXRhWE1QPD94cGFja2V0IGJlZ2luPSLvu78iIGlkPSJXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQiPz4gPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iQWRvYmUgWE1QIENvcmUgNS4wLWMwNjAgNjEuMTM0Nzc3LCAyMDEwLzAyLzEyLTE3OjMyOjAwICAgICAgICAiPiA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPiA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIgeG1wTU06T3JpZ2luYWxEb2N1bWVudElEPSJ4bXAuZGlkOjAxODAxMTc0MDcyMDY4MTE4OEM2REYyN0ExMDhBNDJFIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOjdCNTAyODcwMEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOjdCNTAyODZGMEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCBDUzUgTWFjaW50b3NoIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6MDE4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6MDE4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz4B//79/Pv6+fj39vX08/Lx8O/u7ezr6uno5+bl5OPi4eDf3t3c29rZ2NfW1dTT0tHQz87NzMvKycjHxsXEw8LBwL++vby7urm4t7a1tLOysbCvrq2sq6qpqKempaSjoqGgn56dnJuamZiXlpWUk5KRkI+OjYyLiomIh4aFhIOCgYB/fn18e3p5eHd2dXRzcnFwb25tbGtqaWhnZmVkY2JhYF9eXVxbWllYV1ZVVFNSUVBPTk1MS0pJSEdGRURDQkFAPz49PDs6OTg3NjU0MzIxMC8uLSwrKikoJyYlJCMiISAfHh0cGxoZGBcWFRQTEhEQDw4NDAsKCQgHBgUEAwIBAAAh+QQBAAABACwAAAAAFQAJAAACF4yPgMsJ2mJ4VDKKrd4GVz5lYPeMiVUAADs=);background-image:url(http://robotica.unileon.es/mediawiki/resources/jquery/images/sort_both.gif?2014-04-10T08:01:40Z)!ie;cursor:pointer;background-repeat:no-repeat;background-position:center right;padding-right:21px}table.jquery-tablesorter th.headerSortUp{background-image:url(data:image/gif;base64,R0lGODlhFQAEAIABAAAAAAAAACH/C1hNUCBEYXRhWE1QPD94cGFja2V0IGJlZ2luPSLvu78iIGlkPSJXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQiPz4gPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iQWRvYmUgWE1QIENvcmUgNS4wLWMwNjAgNjEuMTM0Nzc3LCAyMDEwLzAyLzEyLTE3OjMyOjAwICAgICAgICAiPiA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPiA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIgeG1wTU06T3JpZ2luYWxEb2N1bWVudElEPSJ4bXAuZGlkOjAzODAxMTc0MDcyMDY4MTE4OEM2REYyN0ExMDhBNDJFIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOjdCNTAyODc0MEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOjdCNTAyODczMEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCBDUzUgTWFjaW50b3NoIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6MDM4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6MDM4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz4B//79/Pv6+fj39vX08/Lx8O/u7ezr6uno5+bl5OPi4eDf3t3c29rZ2NfW1dTT0tHQz87NzMvKycjHxsXEw8LBwL++vby7urm4t7a1tLOysbCvrq2sq6qpqKempaSjoqGgn56dnJuamZiXlpWUk5KRkI+OjYyLiomIh4aFhIOCgYB/fn18e3p5eHd2dXRzcnFwb25tbGtqaWhnZmVkY2JhYF9eXVxbWllYV1ZVVFNSUVBPTk1MS0pJSEdGRURDQkFAPz49PDs6OTg3NjU0MzIxMC8uLSwrKikoJyYlJCMiISAfHh0cGxoZGBcWFRQTEhEQDw4NDAsKCQgHBgUEAwIBAAAh+QQBAAABACwAAAAAFQAEAAACDYwfoAvoz9qbZ9FrJC0AOw==);background-image:url(http://robotica.unileon.es/mediawiki/resources/jquery/images/sort_up.gif?2014-04-10T08:01:40Z)!ie}table.jquery-tablesorter th.headerSortDown{background-image:url(data:image/gif;base64,R0lGODlhFQAEAIABAAAAAAAAACH/C1hNUCBEYXRhWE1QPD94cGFja2V0IGJlZ2luPSLvu78iIGlkPSJXNU0wTXBDZWhpSHpyZVN6TlRjemtjOWQiPz4gPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iQWRvYmUgWE1QIENvcmUgNS4wLWMwNjAgNjEuMTM0Nzc3LCAyMDEwLzAyLzEyLTE3OjMyOjAwICAgICAgICAiPiA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPiA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtbG5zOnhtcD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wLyIgeG1wTU06T3JpZ2luYWxEb2N1bWVudElEPSJ4bXAuZGlkOjAyODAxMTc0MDcyMDY4MTE4OEM2REYyN0ExMDhBNDJFIiB4bXBNTTpEb2N1bWVudElEPSJ4bXAuZGlkOjhFNzNGQjI3MEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOjhFNzNGQjI2MEY4NjExRTBBMzkyQzAyM0E1RDk3RDc3IiB4bXA6Q3JlYXRvclRvb2w9IkFkb2JlIFBob3Rvc2hvcCBDUzUgTWFjaW50b3NoIj4gPHhtcE1NOkRlcml2ZWRGcm9tIHN0UmVmOmluc3RhbmNlSUQ9InhtcC5paWQ6MDI4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiIHN0UmVmOmRvY3VtZW50SUQ9InhtcC5kaWQ6MDI4MDExNzQwNzIwNjgxMTg4QzZERjI3QTEwOEE0MkUiLz4gPC9yZGY6RGVzY3JpcHRpb24+IDwvcmRmOlJERj4gPC94OnhtcG1ldGE+IDw/eHBhY2tldCBlbmQ9InIiPz4B//79/Pv6+fj39vX08/Lx8O/u7ezr6uno5+bl5OPi4eDf3t3c29rZ2NfW1dTT0tHQz87NzMvKycjHxsXEw8LBwL++vby7urm4t7a1tLOysbCvrq2sq6qpqKempaSjoqGgn56dnJuamZiXlpWUk5KRkI+OjYyLiomIh4aFhIOCgYB/fn18e3p5eHd2dXRzcnFwb25tbGtqaWhnZmVkY2JhYF9eXVxbWllYV1ZVVFNSUVBPTk1MS0pJSEdGRURDQkFAPz49PDs6OTg3NjU0MzIxMC8uLSwrKikoJyYlJCMiISAfHh0cGxoZGBcWFRQTEhEQDw4NDAsKCQgHBgUEAwIBAAAh+QQBAAABACwAAAAAFQAEAAACDYyPAcmtsJyDVDKKWQEAOw==);background-image:url(http://robotica.unileon.es/mediawiki/resources/jquery/images/sort_down.gif?2014-04-10T08:01:40Z)!ie}
/* cache key: wikidb:resourceloader:filter:minify-css:7:34c32de969100a599bb8fb60ec34add1 */</style><meta name="ResourceLoaderDynamicStyles" content="">
<style>a:lang(ar),a:lang(ckb),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}
/* cache key: wikidb:resourceloader:filter:minify-css:7:c2f15db357ed1b34653e004540bd5f70 */</style>

<script src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/load.php"></script><script src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/load(1).php"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)","wgTitle":"PCL/OpenNI tutorial 4: 3D object recognition (descriptors)","wgCurRevisionId":4690,"wgRevisionId":4690,"wgArticleId":334,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)","wgIsProbablyEditable":false,"wgRestrictionEdit":["sysop"],"wgRestrictionMove":["sysop"]});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function(){mw.user.options.set({"ccmeonemails":0,"cols":80,"date":"default","diffonly":0,"disablemail":0,"disablesuggest":0,"editfont":"default","editondblclick":0,"editsection":1,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":0,"extendwatchlist":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"imagesize":2,"justify":0,"math":1,"minordefault":0,"newpageshidepatrolled":0,"nocache":0,"noconvertlink":0,"norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"rcdays":7,"rclimit":50,"rememberpassword":0,"rows":25,"searchlimit":20,"showhiddencats":0,"shownumberswatching":1,"showtoc":1,"showtoolbar":1,"skin":"vector","stubthreshold":0,"thumbsize":2,"underline":2,"uselivepreview":0,"usenewrc":0,"vector-simplesearch":1,"watchcreations":0,"watchdefault":0,"watchdeletion":0,"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,
"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,"watchlisthidepatrolled":0,"watchmoves":0,"wllimit":250,"useeditwarning":1,"prefershttps":1,"language":"en","variant-gan":"gan","variant-iu":"iu","variant-kk":"kk","variant-ku":"ku","variant-shi":"shi","variant-sr":"sr","variant-tg":"tg","variant-uz":"uz","variant-zh":"zh","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":false,"searchNs15":false,"variant":"en"});},{},{});mw.loader.implement("user.tokens",function(){mw.user.tokens.set({"editToken":"+\\","patrolToken":false,"watchToken":false});},{},{});
/* cache key: wikidb:resourceloader:filter:minify-js:7:5a69c7e0fa6557851b6dea8a62efaffb */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax","skins.vector.js"]);
}</script><script src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/load(2).php"></script>
<style type="text/css">/*<![CDATA[*/
.source-cpp {line-height: normal;}
.source-cpp li, .source-cpp pre {
	line-height: normal; border: 0px none white;
}
/**
 * GeSHi Dynamically Generated Stylesheet
 * --------------------------------------
 * Dynamically generated stylesheet for cpp
 * CSS class: source-cpp, CSS id: 
 * GeSHi (C) 2004 - 2007 Nigel McNie, 2007 - 2008 Benny Baumann
 * (http://qbnz.com/highlighter/ and http://geshi.org/)
 * --------------------------------------
 */
.cpp.source-cpp .de1, .cpp.source-cpp .de2 {font: normal normal 1em/1.2em monospace; margin:0; padding:0; background:none; vertical-align:top;}
.cpp.source-cpp  {font-family:monospace;}
.cpp.source-cpp .imp {font-weight: bold; color: red;}
.cpp.source-cpp li, .cpp.source-cpp .li1 {font-weight: normal; vertical-align:top;}
.cpp.source-cpp .ln {width:1px;text-align:right;margin:0;padding:0 2px;vertical-align:top;}
.cpp.source-cpp .li2 {font-weight: bold; vertical-align:top;}
.cpp.source-cpp .kw1 {color: #0000ff;}
.cpp.source-cpp .kw2 {color: #0000ff;}
.cpp.source-cpp .kw3 {color: #0000dd;}
.cpp.source-cpp .kw4 {color: #0000ff;}
.cpp.source-cpp .co1 {color: #666666;}
.cpp.source-cpp .co2 {color: #339900;}
.cpp.source-cpp .coMULTI {color: #ff0000; font-style: italic;}
.cpp.source-cpp .es0 {color: #000099; font-weight: bold;}
.cpp.source-cpp .es1 {color: #000099; font-weight: bold;}
.cpp.source-cpp .es2 {color: #660099; font-weight: bold;}
.cpp.source-cpp .es3 {color: #660099; font-weight: bold;}
.cpp.source-cpp .es4 {color: #660099; font-weight: bold;}
.cpp.source-cpp .es5 {color: #006699; font-weight: bold;}
.cpp.source-cpp .br0 {color: #008000;}
.cpp.source-cpp .sy0 {color: #008000;}
.cpp.source-cpp .sy1 {color: #000080;}
.cpp.source-cpp .sy2 {color: #000040;}
.cpp.source-cpp .sy3 {color: #000040;}
.cpp.source-cpp .sy4 {color: #008080;}
.cpp.source-cpp .st0 {color: #FF0000;}
.cpp.source-cpp .nu0 {color: #0000dd;}
.cpp.source-cpp .nu6 {color: #208080;}
.cpp.source-cpp .nu8 {color: #208080;}
.cpp.source-cpp .nu12 {color: #208080;}
.cpp.source-cpp .nu16 {color:#800080;}
.cpp.source-cpp .nu17 {color:#800080;}
.cpp.source-cpp .nu18 {color:#800080;}
.cpp.source-cpp .nu19 {color:#800080;}
.cpp.source-cpp .me1 {color: #007788;}
.cpp.source-cpp .me2 {color: #007788;}
.cpp.source-cpp .ln-xtra, .cpp.source-cpp li.ln-xtra, .cpp.source-cpp div.ln-xtra {background-color: #ffc;}
.cpp.source-cpp span.xtra { display:block; }

/*]]>*/
</style><style type="text/css">/*<![CDATA[*/
.source-bash {line-height: normal;}
.source-bash li, .source-bash pre {
	line-height: normal; border: 0px none white;
}
/**
 * GeSHi Dynamically Generated Stylesheet
 * --------------------------------------
 * Dynamically generated stylesheet for bash
 * CSS class: source-bash, CSS id: 
 * GeSHi (C) 2004 - 2007 Nigel McNie, 2007 - 2008 Benny Baumann
 * (http://qbnz.com/highlighter/ and http://geshi.org/)
 * --------------------------------------
 */
.bash.source-bash .de1, .bash.source-bash .de2 {font: normal normal 1em/1.2em monospace; margin:0; padding:0; background:none; vertical-align:top;}
.bash.source-bash  {font-family:monospace;}
.bash.source-bash .imp {font-weight: bold; color: red;}
.bash.source-bash li, .bash.source-bash .li1 {font-weight: normal; vertical-align:top;}
.bash.source-bash .ln {width:1px;text-align:right;margin:0;padding:0 2px;vertical-align:top;}
.bash.source-bash .li2 {font-weight: bold; vertical-align:top;}
.bash.source-bash .kw1 {color: #000000; font-weight: bold;}
.bash.source-bash .kw2 {color: #c20cb9; font-weight: bold;}
.bash.source-bash .kw3 {color: #7a0874; font-weight: bold;}
.bash.source-bash .co0 {color: #666666; font-style: italic;}
.bash.source-bash .co1 {color: #800000;}
.bash.source-bash .co2 {color: #cc0000; font-style: italic;}
.bash.source-bash .co3 {color: #000000; font-weight: bold;}
.bash.source-bash .es1 {color: #000099; font-weight: bold;}
.bash.source-bash .es2 {color: #007800;}
.bash.source-bash .es3 {color: #007800;}
.bash.source-bash .es4 {color: #007800;}
.bash.source-bash .es5 {color: #780078;}
.bash.source-bash .es_h {color: #000099; font-weight: bold;}
.bash.source-bash .br0 {color: #7a0874; font-weight: bold;}
.bash.source-bash .sy0 {color: #000000; font-weight: bold;}
.bash.source-bash .st0 {color: #ff0000;}
.bash.source-bash .st_h {color: #ff0000;}
.bash.source-bash .nu0 {color: #000000;}
.bash.source-bash .re0 {color: #007800;}
.bash.source-bash .re1 {color: #007800;}
.bash.source-bash .re2 {color: #007800;}
.bash.source-bash .re4 {color: #007800;}
.bash.source-bash .re5 {color: #660033;}
.bash.source-bash .ln-xtra, .bash.source-bash li.ln-xtra, .bash.source-bash div.ln-xtra {background-color: #ffc;}
.bash.source-bash span.xtra { display:block; }

/*]]>*/
</style><!--[if lt IE 7]><style type="text/css">body{behavior:url("/mediawiki/skins/vector/csshover.min.htc")}</style><![endif]--><script async="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/load(3).php"></script><script async="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/load(4).php"></script></head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-PCL_OpenNI_tutorial_4_3D_object_recognition_descriptors skin-vector action-view vector-animateLayout">
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="mw-js-message" style="display:none;"></div>
						<h1 id="firstHeading" class="firstHeading" lang="en"><span dir="auto">PCL/OpenNI tutorial 4: 3D object recognition (descriptors)</span></h1>
			<div id="bodyContent">
								<div id="siteSub">From Robótica - ULE</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#mw-navigation">navigation</a>, 					<a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><p>Go to root: <a href="http://robotica.unileon.es/mediawiki/index.php/PhD-3D-Object-Tracking" title="PhD-3D-Object-Tracking">PhD-3D-Object-Tracking</a>
</p>
<hr>
<hr>
<p><br>
It is time to learn the basics of one of the most interesting applications of point cloud processing: 3D object recognition. Akin to 2D recognition, this technique relies on finding good keypoints (characteristic points) in the cloud, and matching them to a set of previously saved ones. But 3D has several advantages over 2D: namely, we will be able to estimate with decent accuracy the exact position and orientation of the object, relative to the sensor. Also, 3D object recognition tends to be more robust to clutter (crowded scenes where objects in the front occluding objects in the background). And finally, having information about the object's shape will help with collision avoidance or grasping operations.
</p><p>In this first tutorial we will see what descriptors are, how many types are there available in PCL, and how to compute them.
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2><span class="toctoggle">&nbsp;[<a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#" class="internal" id="togglelink">hide</a>]&nbsp;</span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Table"><span class="tocnumber">1.1</span> <span class="toctext">Table</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-3"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Local_descriptors"><span class="tocnumber">2</span> <span class="toctext">Local descriptors</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PFH"><span class="tocnumber">2.1</span> <span class="toctext">PFH</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#FPFH"><span class="tocnumber">2.1.1</span> <span class="toctext">FPFH</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-6"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RSD"><span class="tocnumber">2.2</span> <span class="toctext">RSD</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#3DSC"><span class="tocnumber">2.3</span> <span class="toctext">3DSC</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#USC"><span class="tocnumber">2.3.1</span> <span class="toctext">USC</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#SHOT"><span class="tocnumber">2.4</span> <span class="toctext">SHOT</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Spin_image"><span class="tocnumber">2.5</span> <span class="toctext">Spin image</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RIFT"><span class="tocnumber">2.6</span> <span class="toctext">RIFT</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#NARF"><span class="tocnumber">2.7</span> <span class="toctext">NARF</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Obtaining_a_range_image"><span class="tocnumber">2.7.1</span> <span class="toctext">Obtaining a range image</span></a>
<ul>
<li class="toclevel-4 tocsection-14"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Spherical_projection"><span class="tocnumber">2.7.1.1</span> <span class="toctext">Spherical projection</span></a></li>
<li class="toclevel-4 tocsection-15"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Planar_projection"><span class="tocnumber">2.7.1.2</span> <span class="toctext">Planar projection</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-16"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Extracting_borders"><span class="tocnumber">2.7.2</span> <span class="toctext">Extracting borders</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Finding_keypoints"><span class="tocnumber">2.7.3</span> <span class="toctext">Finding keypoints</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Computing_the_descriptor"><span class="tocnumber">2.7.4</span> <span class="toctext">Computing the descriptor</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-19"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RoPS"><span class="tocnumber">2.8</span> <span class="toctext">RoPS</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-20"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Global_descriptors"><span class="tocnumber">3</span> <span class="toctext">Global descriptors</span></a>
<ul>
<li class="toclevel-2 tocsection-21"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#VFH"><span class="tocnumber">3.1</span> <span class="toctext">VFH</span></a>
<ul>
<li class="toclevel-3 tocsection-22"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#CVFH"><span class="tocnumber">3.1.1</span> <span class="toctext">CVFH</span></a>
<ul>
<li class="toclevel-4 tocsection-23"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#OUR-CVFH"><span class="tocnumber">3.1.1.1</span> <span class="toctext">OUR-CVFH</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-2 tocsection-24"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#ESF"><span class="tocnumber">3.2</span> <span class="toctext">ESF</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#GFPFH"><span class="tocnumber">3.3</span> <span class="toctext">GFPFH</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#GRSD"><span class="tocnumber">3.4</span> <span class="toctext">GRSD</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Visualization"><span class="tocnumber">4</span> <span class="toctext">Visualization</span></a>
<ul>
<li class="toclevel-2 tocsection-28"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PCLHistogramVisualizer"><span class="tocnumber">4.1</span> <span class="toctext">PCLHistogramVisualizer</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PCLPlotter"><span class="tocnumber">4.2</span> <span class="toctext">PCLPlotter</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PCL_Viewer"><span class="tocnumber">4.3</span> <span class="toctext">PCL Viewer</span></a></li>
</ul>
</li>
</ul>
</div>

<h1><span class="mw-headline" id="Overview">Overview</span></h1>
<p>The basis of 3D object recognition is to find a set of correspondences between two different clouds, one of them containing the object we are looking for. In order to do this, we need a way to compare points in an unambiguous manner. Until now, we have worked with points that store the XYZ coordinates, the RGB color... but none of those properties are unique enough. In two sequential scans, two points could share the same coordinates despite belonging to different surfaces, and using the color information takes us back to 2D recognition, will all the lightning related problems.
</p><p>In a previous tutorial, we talked about <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_2:_Cloud_processing_(basic)#Feature_estimation" title="PCL/OpenNI tutorial 2: Cloud processing (basic)"> features</a>, before introducing the normals. Normals are an example of feature, because they encode information about the vicinity of the point. That is, the neighboring points are taken into account when computing it, giving us an idea of how the surrounding surface is. But this is not enough. For a feature to be optimal, it must meet the following criteria:
</p>
<ol>
<li> <b>It must be robust to transformations</b>: rigid transformations (the ones that do not change the distance between points) like translations and rotations must not affect the feature. Even if we play with the cloud a bit beforehand, there should be no difference.
</li>
<li> <b>It must be robust to noise</b>: measurement errors that cause noise should not change the feature estimation much.
</li>
<li> <b>It must be resolution invariant</b>: if sampled with different density (like after performing downsampling), the result must be identical or similar.
</li>
</ol>
<p>This is where descriptors come in. They are more complex (and precise) signatures of a point, that encode a lot of information about the surrounding geometry. The purpose is to unequivocally identify a point across multiple point clouds, no matter the noise, resolution or transformations. Also, some of them capture additional data about the object they belong to, like the viewpoint (that lets us retrieve the pose).
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:602px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Correspondence.jpg" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Correspondence.jpg" width="600" height="191" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Correspondence.jpg" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Finding correspondences between point features of two clouds (image from <a rel="nofollow" class="external free" href="http://pointclouds.org/">http://pointclouds.org/</a>).</div></div></div></div>
<p><br>
There are many 3D descriptors implemented into PCL. Each one has its own method for computing unique values for a point. Some use the difference between the angles of the normals of the point and its neighbors, for example. Others use the distances between the points. Because of this, some are inherently better or worse for certain purposes. A given descriptor may be scale invariant, and another one may be better with occlusions and partial views of objects. Which one you choose depends on what you want to do.
</p><p>After calculating the necessary values, an additional step is performed to reduce the descriptor size: the result is binned into an <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Histogram">histogram</a>. To do this, the value range of each variable that makes up the descriptor is divided into <i>n</i> subdivisions, and the number of occurrences in each one is counted. Try to imagine a descriptor that computes a single variable, that ranges from 1 to 100. We choose to create 10 bins for it, so the first bin would gather all occurrences between 1 and 10, the second from 11 to 20, and so on. We look at the value of the variable for the first point-neighbor pair, and it is 27, so we increment the value of the third bin by 1. We keep doing this until we get a final histogram for that keypoint. The bin size must be carefully chosen depending on how descriptive that variable is (the variables do not have to share the same number of bins, and also the bins do not have to be of the same size; if for example most values from the previous example fell in the 50-100 range then it would be sensible to have more bins of smaller size in that range).
</p><p>Descriptors can be classified in two main categories: global and local. The process for computing and using each one (recognition pipeline) is different, so each will be explained in its own section in this article.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Tutorials</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/how_features_work.php">How 3D Features work in PCL</a>
</li>
<li> <a rel="nofollow" class="external text" href="https://github.com/PointCloudLibrary/pcl/wiki/Overview-and-Comparison-of-Features">Overview and Comparison of Features</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://www.pointclouds.org/assets/icra2013/pcl_features_icra13.pdf">How does a good feature look like?</a>
</li>
</ul>
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://www.inf.ethz.ch/personal/zeislb/publications/aldoma_2012jram_PCLTutorial.pdf">Point Cloud Library: Three-Dimensional Object Recognition and 6 DoF Pose Estimation</a> (Aitor Aldoma et al., 2012)
</li>
</ul>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="Table">Table</span></h2>
<p>The following table will give you a hint of how many descriptors are there in PCL, and some of their features:
</p>
<table class="wikitable sortable jquery-tablesorter" style="margin: auto; text-align:center" cellpadding="15">
<caption> 3D descriptors
</caption>
<thead><tr style="vertical-align:middle;">
<th scope="col" class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending"> Name
</th>
<th scope="col" class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending"> Type
</th>
<th scope="col" class="headerSort" tabindex="0" role="columnheader button" title="Sort ascending"> Size
</th></tr></thead><tbody>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">PFH</a> (Point Feature Histogram)
</td>
<td style="color: red;"> Local
</td>
<td> 125
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#FPFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">FPFH</a> (Fast Point Feature Histogram)
</td>
<td style="color: red;"> Local
</td>
<td> 33
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RSD" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">RSD</a> (Radius-Based Surface Descriptor)
</td>
<td style="color: red;"> Local
</td>
<td> 289
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#3DSC" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">3DSC</a> (3D Shape Context)
</td>
<td style="color: red;"> Local
</td>
<td> 1980
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#USC" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">USC</a> (Unique Shape Context)
</td>
<td style="color: red;"> Local
</td>
<td> 1960
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#SHOT" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">SHOT</a> (Signatures of Histograms of Orientations)
</td>
<td style="color: red;"> Local
</td>
<td> 352
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Spin_image" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">Spin image</a>
</td>
<td style="color: red;"> Local
</td>
<td> 153*
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RIFT" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">RIFT</a> (Rotation-Invariant Feature Transform)
</td>
<td style="color: red;"> Local
</td>
<td> 32*
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#NARF" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">NARF</a> (Normal Aligned Radial Feature)
</td>
<td style="color: red;"> Local
</td>
<td> 36
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#RoPS" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">RoPS</a> (Rotational Projection Statistics)
</td>
<td style="color: red;"> Local
</td>
<td> 135*
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#VFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">VFH</a> (Viewpoint Feature Histogram)
</td>
<td style="color: green;"> Global
</td>
<td> 308
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#CVFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">CVFH</a> (Clustered Viewpoint Feature Histogram)
</td>
<td style="color: green;"> Global
</td>
<td> 308
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#OUR-CVFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">OUR-CVFH</a> (Oriented, Unique and Repeatable Clustered Viewpoint Feature Histogram)
</td>
<td style="color: green;"> Global
</td>
<td> 308
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#ESF" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">ESF</a> (Ensemble of Shape Functions)
</td>
<td style="color: green;"> Global
</td>
<td> 640
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#GFPFH" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">GFPFH</a> (Global Fast Point Feature Histogram)
</td>
<td style="color: green;"> Global
</td>
<td> 16
</td></tr>
<tr style="vertical-align:middle;">
<td> <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#GRSD" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">GRSD</a> (Global Radius-Based Surface Descriptor)
</td>
<td style="color: green;"> Global
</td>
<td> 21
</td></tr></tbody><tfoot></tfoot></table>
<p><br>
Values marked with an asterisk (*) indicate that the descriptor's size depends on some parameter(s), and the one given is for the default values.
</p><p>Optionally, you can download a document with a less simple version of the table. Page format is A4, landscape:
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Downloads</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/3D_descriptors_comparison.odt">Table as original ODT</a> (uses font embedding, requires LibreOffice 4)
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/3D_descriptors_comparison.pdf">Table as PDF</a>
</li>
</ul>
</li>
</ul>
</div>
<p><br>
</p>
<h1><span class="mw-headline" id="Local_descriptors">Local descriptors</span></h1>
<p>Local descriptors are computed for individual points that we give as input. They have no notion of what an object is, they just describe how the local geometry is around that point. Usually, it is your task to choose which points you want a descriptor to be computed for: the <i>keypoints</i>. Most of the time, you can get away by just performing a downsampling and choosing all remaining points, but keypoint detectors are available, like the one used for <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#Finding_keypoints" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">NARF</a>, or <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_5:_3D_object_recognition_(pipeline)#ISS" title="PCL/OpenNI tutorial 5: 3D object recognition (pipeline)">ISS</a>.
</p><p>Local descriptors are used for object recognition and <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_3:_Cloud_processing_(advanced)#Registration" title="PCL/OpenNI tutorial 3: Cloud processing (advanced)"> registration</a>. Now we will see which ones are implemented into PCL.
</p>
<h2><span class="mw-headline" id="PFH">PFH</span></h2>
<p>PFH stands for Point Feature Histogram. It is one of the most important descriptors offered by PCL and the basis of others such as FPFH. The PFH tries to capture information of the geometry surrounding the point by analyzing the difference between the directions of the normals in the vicinity (and because of this, an imprecise normal estimation may produce low-quality descriptors).
</p><p>First, the algorithm pairs all points in the vicinity (not just the chosen keypoint with its neighbors, but also the neighbors with themselves). Then, for each pair, a <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Darboux_frame">fixed coordinate frame</a> is computed from their normals. With this frame, the difference between the normals can be encoded with 3 angular variables. These variables, together with the euclidean distance between the points, are saved, and then binned to an histogram when all pairs have been computed. The final descriptor is the concatenation of the histograms of each variable (4 in total).
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PFH_neighbors.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PFH_neighbors.png" width="127" height="120"></a></div></div>
			<div class="gallerytext">
<p>Point pairs established when computing the PFH for a point (image from <a rel="nofollow" class="external free" href="http://pointclouds.org/">http://pointclouds.org</a>).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PFH_frame.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PFH_frame.png" width="293" height="120"></a></div></div>
			<div class="gallerytext">
<p>Fixed coordinate frame and angular features computed for one of the pairs (image from <a rel="nofollow" class="external free" href="http://pointclouds.org/">http://pointclouds.org</a>).
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
Computing descriptors in PCL is very easy, and the PFH is not an exception:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/pfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the PFH descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PFHSignature125</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PFHSignature125</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// PFH estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">PFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">PFHSignature125</span><span class="sy1">&gt;</span> pfh<span class="sy4">;</span>
	pfh.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	pfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	pfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. Note: the value given here has to be</span>
	<span class="co1">// larger than the radius used to estimate the normals.</span>
	pfh.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	pfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>As you can see, PCL uses the <span style="color:#FF1493">"PFHSignature125"</span> type to save the descriptor to. This means that the descriptor's size is 125 (the dimensionality of the feature vector). Dividing a feature in <i>D</i> dimensional space in <i>B</i> divisions requires a total of B<sup>D</sup> bins. The original proposal makes use of the distance between the points, but the implementation of PCL does not, as it was not considered discriminative enough (specially in 2.5D scans, where the distance between points increases the further away from the sensor). Hence, the remaining features (with one dimension each) can be divided in 5 divisions, resulting in a 125-dimensional (5<sup>3</sup>) vector.
</p><p>The final object that stores the computed descriptors can be handled like a normal cloud (even saved to, or read from, disk), and it has the same number of "points" than the original one. The <span style="color:#FF1493">"PFHSignature125"</span> object at position 0, for example, stores the PFH descriptor for the <span style="color:#FF1493">"PointXYZ"</span> point at the same position in the cloud.
</p><p>For additional details about the descriptor, check the original publications listed below, or PCL's tutorial.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Search method, Radius
</li>
<li> <b>Output</b>: PFH descriptors
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/pfh_estimation.php">Point Feature Histograms (PFH) descriptors</a>
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://ias.in.tum.de/_media/spezial/bib/rusu08ias.pdf">Persistent Point Feature Histograms for 3D Point Clouds</a> (Radu Bogdan Rusu et al., 2008)
</li>
<li> <a rel="nofollow" class="external text" href="http://files.rbrusu.com/publications/RusuPhDThesis.pdf">Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments</a> (Radu Bogdan Rusu, 2009, section 4.4, page 50)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_p_f_h_estimation.html">pcl::PFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_PFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h3><span class="mw-headline" id="FPFH">FPFH</span></h3>
<p>PFH gives accurate results, but it has a drawback: it is too computationally expensive to perform at real time. For a cloud of <i>n</i> keypoints with <i>k</i> neighbors considered, it has a <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">complexity</a> of <i>O(nk<sup>2</sup>)</i>. Because of this, a derived descriptor was created, named FPFH (Fast Point Feature Histogram).
</p><p>The FPFH considers only the direct connections between the current keypoint and its neighbors, removing additional links between neighbors. This takes the complexity down to <i>O(nk)</i>. Because of this, the resulting histogram is referred to as SPFH (Simplified Point Feature Histogram). The reference frame and the angular variables are computed as always.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:315px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:FPFH_neighbors.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/FPFH_neighbors.png" width="313" height="241" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:FPFH_neighbors.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Point pairs established when computing the FPFH for a point (image from <a rel="nofollow" class="external free" href="http://pointclouds.org/">http://pointclouds.org</a>).</div></div></div></div>
<p><br>
To account for the loss of these extra connections, an additional step takes place after all histograms have been computed: the SPFHs of a point's neighbors are "merged" with its own, weighted according to the distance. This has the effect of giving a point surface information of points as far away as 2 times the radius used. Finally, the 3 histograms (distance is not used) are concatenated to compose the final descriptor.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/fpfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the FPFH descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">FPFHSignature33</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">FPFHSignature33</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// FPFH estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">FPFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">FPFHSignature33</span><span class="sy1">&gt;</span> fpfh<span class="sy4">;</span>
	fpfh.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	fpfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	fpfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. Note: the value given here has to be</span>
	<span class="co1">// larger than the radius used to estimate the normals.</span>
	fpfh.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	fpfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>An additional implementation of the FPFH estimation that takes advantage of multithreaded optimizations (with the <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/OpenMP">OpenMP API</a>) is available in the class <span style="color:#FF1493">"FPFHEstimationOMP"</span>. Its interface is identical to the standard unoptimized implementation. Using it will result in a big performance boost on multi-core systems, meaning faster computation times. Remember to include the header <span style="color:#FF1493">"fpfh_omp.h"</span> instead.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Search method, Radius
</li>
<li> <b>Output</b>: FPFH descriptors
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/fpfh_estimation.php">Fast Point Feature Histograms (FPFH) descriptors</a>
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://files.rbrusu.com/publications/Rusu09ICRA.pdf">Fast Point Feature Histograms (FPFH) for 3D Registration</a> (Radu Bogdan Rusu et al., 2009)
</li>
<li> <a rel="nofollow" class="external text" href="http://files.rbrusu.com/publications/RusuPhDThesis.pdf">Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments</a> (Radu Bogdan Rusu, 2009, section 4.5, page 57)
</li>
</ul>
</li>
<li> <b>API</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_f_p_f_h_estimation.html">pcl::FPFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_f_p_f_h_estimation_o_m_p.html">pcl::FPFHEstimationOMP</a>
</li>
</ul>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_FPFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="RSD">RSD</span></h2>
<p>The Radius-Based Surface Descriptor encodes the radial relationship of the point and its neighborhood. For every pair of the keypoint with a neighbor, the algorithm computes the distance between them, and the difference between their normals. Then, by assuming that both points lie on the surface of a sphere, said sphere is found by fitting not only the points, but also the normals (otherwise, there would be infinite possible spheres). Finally, from all the point-neighbor spheres, only the ones with the maximum and minimum radii are kept and saved to the descriptor of that point.
</p><p>As you may have deduced already, when two points lie on a flat surface, the sphere radius will be infinite. If, on the other hand, they lie on the curved face of a cylinder, the radius will be more or less the same as that of the cylinder. This allows us to tell objects apart with RSD. The algorithm takes a parameter that sets the maximum radius at which the points will be considered to be part of a plane.
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:20px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:RSD_sphere.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/RSD_sphere.png" width="300" height="110"></a></div></div>
			<div class="gallerytext">
<p>Sphere that fits two points with their normals (image from <a rel="nofollow" class="external text" href="http://ias.in.tum.de/_media/events/rgbd2011/marton.pdf">original presentation</a>).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:RSD_values.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/RSD_values.png" width="180" height="120"></a></div></div>
			<div class="gallerytext">
<p>RSD values for a cloud; points on a flat surface have maximum values (image from <a rel="nofollow" class="external text" href="http://ias.in.tum.de/_media/events/rgbd2011/marton.pdf">original presentation</a>).
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
This is the code for compiling the RSD descriptor:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/rsd.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the RSD descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PrincipalRadiiRSD</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PrincipalRadiiRSD</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// RSD estimation object.</span>
	RSDEstimation<span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">PrincipalRadiiRSD</span><span class="sy1">&gt;</span> rsd<span class="sy4">;</span>
	rsd.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	rsd.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	rsd.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. Note: the value given here has to be</span>
	<span class="co1">// larger than the radius used to estimate the normals.</span>
	rsd.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Plane radius. Any radius larger than this is considered infinite (a plane).</span>
	rsd.<span class="me1">setPlaneRadius</span><span class="br0">(</span>0.1<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Do we want to save the full distance-angle histograms?</span>
	rsd.<span class="me1">setSaveHistograms</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	rsd.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p><span style="color:#606060"><i><b>NOTE: This code will only compile with PCL versions 1.8 and above (the current trunk).</b></i></span>
</p><p>Optionally, you can use the <span style="color:#FF1493">"setSaveHistograms()"</span> function to enable the saving of the full distance-angle histograms, and then use <span style="color:#FF1493">"getHistograms()"</span> to retrieve them.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Search method, Radius, Maximum Radius, [Subdivisions], [Save full histograms]
</li>
<li> <b>Output</b>: RSD descriptors
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.391.7398&rep=rep1&type=pdf">General 3D Modelling of Novel Objects from a Single View</a> (Zoltan-Csaba Marton et al., 2010)
</li>
<li> <a rel="nofollow" class="external text" href="https://ias.cs.tum.edu/_media/spezial/bib/marton11ijrr.pdf">Combined 2D-3D Categorization and Classification for Multimodal Perception Systems</a> (Zoltan-Csaba Marton et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_r_s_d_estimation.html">pcl::RSDEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_RSD.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="3DSC">3DSC</span></h2>
<p>The 3D Shape Context is a descriptor that extends its existing 2D counterpart to the third dimension. It works by creating a support structure (a sphere, to be precise) centered at the point we are computing the descriptor for, with the given search radius. The "north pole" of that sphere (the notion of "up") is pointed to match the normal at that point. Then, the sphere is divided in 3D regions or bins. In the first 2 coordinates (azimuth and elevation) the divisions are equally spaced, but in the third (the radial dimension), divisions are logarithmically spaced so they are smaller towards the center. A minimum radius can be specified to prevent very small bins, that would be too sensitive to small changes in the surface.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:202px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:3DSC_support_structure.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/3DSC_support_structure.png" width="200" height="184" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:3DSC_support_structure.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Support structure to compute the 3DSC for a point (image from <a rel="nofollow" class="external text" href="http://www.cs.jhu.edu/~misha/Papers/Frome04.pdf">original paper</a>).</div></div></div></div>
<p><br>
For each bin, a weighted count is accumulated for every neighboring point that lies within. The weight depends on the volume of the bin and the local point density (number of points around the current neighbor). This gives the descriptor some degree of resolution invariance.
</p><p>We have mentioned that the sphere is given the direction of the normal. This still leaves one degree of freedom (only two axes have been locked, the azimuth remains free). Because of this, the descriptor so far does not cope with rotation. To overcome this (so the same point in two different clouds has the same value), the support sphere is rotated around the normal <i>N</i> times (a number of degrees that corresponds with the divisions in the azimuth) and the process is repeated for each, giving a total of <i>N</i> descriptors for that point.
</p><p>You can compute the 3DSC descriptor the following way:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/3dsc.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the 3DSC descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">ShapeContext1980</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">ShapeContext1980</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// 3DSC estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">ShapeContext3DEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">ShapeContext1980</span><span class="sy1">&gt;</span> sc3d<span class="sy4">;</span>
	sc3d.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	sc3d.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	sc3d.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. It will also be the radius of the support sphere.</span>
	sc3d.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// The minimal radius value for the search sphere, to avoid being too sensitive</span>
	<span class="co1">// in bins close to the center of the sphere.</span>
	sc3d.<span class="me1">setMinimalRadius</span><span class="br0">(</span>0.05 <span class="sy2">/</span> 10.0<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Radius used to compute the local point density for the neighbors</span>
	<span class="co1">// (the density is the number of points within that radius).</span>
	sc3d.<span class="me1">setPointDensityRadius</span><span class="br0">(</span>0.05 <span class="sy2">/</span> 5.0<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	sc3d.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Search method, Radius, Minimal radius, Point density radius
</li>
<li> <b>Output</b>: 3DSC descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://www.cs.jhu.edu/~misha/Papers/Frome04.pdf">Recognizing Objects in Range Data Using Regional Point Descriptors</a> (Andrea Frome et al., 2004)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_shape_context3_d_estimation.html">pcl::ShapeContext3DEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_3DSC.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h3><span class="mw-headline" id="USC">USC</span></h3>
<p>The Unique Shape Context descriptor extends the 3DSC by defining a local reference frame, in order to provide an unique orientation for each point. This not only improves the accuracy of the descriptor, it also reduces its size, as computing multiple descriptors to account for orientation is no longer necessary.
</p><p>You can check the second publication listed below to learn more about how the LRF is computed.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/usc.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the USC descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">UniqueShapeContext1960</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">UniqueShapeContext1960</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// USC estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">UniqueShapeContext</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">UniqueShapeContext1960</span>, pcl<span class="sy4">::</span><span class="me2">ReferenceFrame</span><span class="sy1">&gt;</span> usc<span class="sy4">;</span>
	usc.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. It will also be the radius of the support sphere.</span>
	usc.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// The minimal radius value for the search sphere, to avoid being too sensitive</span>
	<span class="co1">// in bins close to the center of the sphere.</span>
	usc.<span class="me1">setMinimalRadius</span><span class="br0">(</span>0.05 <span class="sy2">/</span> 10.0<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Radius used to compute the local point density for the neighbors</span>
	<span class="co1">// (the density is the number of points within that radius).</span>
	usc.<span class="me1">setPointDensityRadius</span><span class="br0">(</span>0.05 <span class="sy2">/</span> 5.0<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the radius to compute the Local Reference Frame.</span>
	usc.<span class="me1">setLocalRadius</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	usc.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p><span style="color:#606060"><i><b>NOTE: This code will only compile with PCL versions 1.8 and above (the current trunk). For 1.7 and below, change UniqueShapeContext1960 to ShapeContext1980, and edit CMakeLists.txt.</b></i></span>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Radius, Minimal radius, Point density radius, Local radius
</li>
<li> <b>Output</b>: USC descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://vision.deis.unibo.it/fede/papers/3dor10.pdf">Unique Shape Context for 3D Data Description</a> (Federico Tombari et al., 2010)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_unique_shape_context.html">pcl::UniqueShapeContext</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_USC.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="SHOT">SHOT</span></h2>
<p>SHOT stands for Signature of Histograms of Orientations. Like 3DSC, it encodes information about the topology (surface) withing a spherical support structure. This sphere is divided in 32 bins or volumes, with 8 divisions along the azimuth, 2 along the elevation, and 2 along the radius. For every volume, a one-dimensional local histogram is computed. The variable chosen is the angle between the normal of the keypoint and the current point within that volume (to be precise, the cosine, which was found to be better suitable).
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:182px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:SHOT_support_structure.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/SHOT_support_structure.png" width="180" height="187" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:SHOT_support_structure.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Support structure to compute SHOT. Only 4 azimuth divisions are shown for clarity (image from <a rel="nofollow" class="external text" href="http://vision.deis.unibo.it/fede/papers/eccv10.pdf">original paper</a>).</div></div></div></div>
<p><br>
When all local histograms have been computed, they are stitched together in a final descriptor. Like the USC descriptor, SHOT makes use of a local reference frame, making it rotation invariant. It is also robust to noise and clutter.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/shot.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the SHOT descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">SHOT352</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">SHOT352</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// SHOT estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">SHOTEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">SHOT352</span><span class="sy1">&gt;</span> shot<span class="sy4">;</span>
	shot.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	shot.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// The radius that defines which of the keypoint's neighbors are described.</span>
	<span class="co1">// If too large, there may be clutter, and if too small, not enough points may be found.</span>
	shot.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.02<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	shot.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>Like with FPFH, a multithreading-optimized variant is available with <span style="color:#FF1493">"SHOTEstimationOMP"</span>, that makes use of OpenMP. You need to include the header <span style="color:#FF1493">"shot_omp.h"</span>. Also, another variant that uses the texture for matching is available, <span style="color:#FF1493">"SHOTColorEstimation"</span>, with an optimized version too (see the second publication for more details). It outputs a <span style="color:#FF1493">"SHOT1344"</span> descriptor.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Radius
</li>
<li> <b>Output</b>: SHOT descriptors
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://vision.deis.unibo.it/fede/papers/eccv10.pdf">Unique Signatures of Histograms for Local Surface Description</a> (Federico Tombari et al., 2010)
</li>
<li> <a rel="nofollow" class="external text" href="http://www.vision.deis.unibo.it/fede/papers/icip11.pdf">A Combined Texture-Shaped Descriptor for Enhanced 3D Feature Matching</a> (Federico Tombari et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_s_h_o_t_estimation.html">pcl::SHOTEstimation</a>, 
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_s_h_o_t_color_estimation.html">pcl::SHOTColorEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_s_h_o_t_estimation_o_m_p.html">pcl::SHOTEstimationOMP</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_s_h_o_t_color_estimation_o_m_p.html">pcl::SHOTColorEstimationOMP</a>
</li>
</ul>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_SHOT.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="Spin_image">Spin image</span></h2>
<p>The Spin Image (SI) is the oldest descriptor we are going to see here. It has been around since 1997, but it still sees some use for certain applications. It was originally designed to describe surfaces made by vertices, edges and polygons, but it has been since adapted for point clouds. The descriptor is unlike all others in that the output resembles an image that can be compared with another with the usual means.
</p><p>The support structure used is a cylinder, centered at the point, with a given radius and height, and aligned with the normal. This cylinder is divided radially and vertically into volumes. For each one, the number of neighbors lying inside is added up, eventually producing a descriptor. Weighting and interpolation are used to improve the result. The final descriptor can be seen as a grayscale image where dark areas correspond to volumes with higher point density.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Spin_images.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Spin_images.png" width="400" height="341" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Spin_images.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Spin images computed for 3 points of a model (image from <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4190&rep=rep1&type=pdf">original thesis</a>).</div></div></div></div>
<p><br>
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/spin_image.h&gt;</span>
&nbsp;
<span class="co1">// A handy typedef.</span>
<span class="kw4">typedef</span> pcl<span class="sy4">::</span><span class="me2">Histogram</span><span class="sy1">&lt;</span>153<span class="sy1">&gt;</span> SpinImage<span class="sy4">;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the spin image for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>SpinImage<span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>SpinImage<span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Spin image estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">SpinImageEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, SpinImage<span class="sy1">&gt;</span> si<span class="sy4">;</span>
	si.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	si.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Radius of the support cylinder.</span>
	si.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.02<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the resolution of the spin image (the number of bins along one dimension).</span>
	<span class="co1">// Note: you must change the output histogram size to reflect this.</span>
	si.<span class="me1">setImageWidth</span><span class="br0">(</span>8<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	si.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>The Spin Image estimation object provides more methods for tuning the estimation, so checking the API is recommended.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Normals, Radius, Image resolution
</li>
<li> <b>Output</b>: Spin images
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.4190&rep=rep1&type=pdf">Spin-Images: A Representation for 3-D Surface Matching</a> (Andrew Edie Johnson, 1997)
</li>
<li> <a rel="nofollow" class="external text" href="http://www.cs.jhu.edu/~misha/Papers/Johnson99.pdf">Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes</a> (Andrew Edie Johnson and Martial Hebert, 1999)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_spin_image_estimation.html">pcl::SpinImageEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_spin_image.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="RIFT">RIFT</span></h2>
<p>The Rotation-Invariant Feature Transform, like the spin image, takes some concepts from 2D features, in this case from the Scale-Invariant Feature Transform (<a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a>). It is the only descriptor seen here that requires intensity information in order to compute it (it can be obtained from the RGB color values). This means, of course, that you will not be able to use RIFT with standard XYZ clouds, you also need the texture.
</p><p>In the first step, a circular patch (with the given radius) is fitted on the surface the point lies on. This patch is divided into concentric rings, according to the chosen distance bin size. Then, an histogram is populated with all the point's neighbors lying inside a sphere centered at that point and with the mentioned radius. The distance and the orientation of the intensity gradient at each point are considered. To make it rotation invariant, the angle between the gradient orientation and the vector pointing outward from the center of the patch is measured.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:602px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:RIFT.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/RIFT.png" width="600" height="307" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:RIFT.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>RIFT feature values at 3 different locations in the descriptor (image from <a rel="nofollow" class="external text" href="http://www.cs.illinois.edu/~slazebni/publications/pami05.pdf">original paper</a>).</div></div></div></div>
<p><br>
The authors' original implementation uses 4 rings and 8 histogram orientations, which produce a descriptor of size 32. RIFT is not robust to texture flipping, though this was never considered a big issue.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/point_types_conversion.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/intensity_gradient.h&gt;</span>
<span class="co2">#include &lt;pcl/features/rift.h&gt;</span>
&nbsp;
<span class="co1">// A handy typedef.</span>
<span class="kw4">typedef</span> pcl<span class="sy4">::</span><span class="me2">Histogram</span><span class="sy1">&lt;</span>32<span class="sy1">&gt;</span> RIFT32<span class="sy4">;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud with color information.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZRGB</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloudColor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZRGB</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the point cloud with intensity value.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloudIntensity<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the intensity gradients.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">IntensityGradient</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> gradients<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">IntensityGradient</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the RIFT descriptor for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>RIFT32<span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>RIFT32<span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZRGB</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloudColor<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Convert the RGB to intensity.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloudXYZRGBtoXYZI</span><span class="br0">(</span><span class="sy2">*</span>cloudColor, <span class="sy2">*</span>cloudIntensity<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloudIntensity<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Compute the intensity gradients.</span>
	pcl<span class="sy4">::</span><span class="me2">IntensityGradientEstimation</span> <span class="sy1">&lt;</span> pcl<span class="sy4">::</span><span class="me2">PointXYZI</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">IntensityGradient</span>,
		pcl<span class="sy4">::</span><span class="me2">common</span><span class="sy4">::</span><span class="me2">IntensityFieldAccessor</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span><span class="sy1">&gt;</span> <span class="sy1">&gt;</span> ge<span class="sy4">;</span>
	ge.<span class="me1">setInputCloud</span><span class="br0">(</span>cloudIntensity<span class="br0">)</span><span class="sy4">;</span>
	ge.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	ge.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	ge.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>gradients<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// RIFT estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">RIFTEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZI</span>, pcl<span class="sy4">::</span><span class="me2">IntensityGradient</span>, RIFT32<span class="sy1">&gt;</span> rift<span class="sy4">;</span>
	rift.<span class="me1">setInputCloud</span><span class="br0">(</span>cloudIntensity<span class="br0">)</span><span class="sy4">;</span>
	rift.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the intensity gradients to use.</span>
	rift.<span class="me1">setInputGradient</span><span class="br0">(</span>gradients<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Radius, to get all neighbors within.</span>
	rift.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.02<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the number of bins to use in the distance dimension.</span>
	rift.<span class="me1">setNrDistanceBins</span><span class="br0">(</span>4<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the number of bins to use in the gradient orientation dimension.</span>
	rift.<span class="me1">setNrGradientBins</span><span class="br0">(</span>8<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Note: you must change the output histogram size to reflect the previous values.</span>
&nbsp;
	rift.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Search method, Intensity gradients, Radius, Distance bins, Gradient bins
</li>
<li> <b>Output</b>: RIFT descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://www.cs.illinois.edu/~slazebni/publications/pami05.pdf">A Sparse Texture Representation Using Local Affine Regions</a> (Svetlana Lazebnik et al., 2005)
</li>
</ul>
</li>
<li> <b>API</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_r_i_f_t_estimation.html">pcl::RIFTEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_intensity_gradient_estimation.html">pcl::IntensityGradientEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/structpcl_1_1_intensity_gradient.html">pcl::IntensityGradient</a>
</li>
</ul>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_RIFT.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="NARF">NARF</span></h2>
<p>The Normal Aligned Radial Feature is the only descriptor here that does not take a point cloud as input. Instead, it works with range images. A range image is a common RGB image in which the distance to the point that corresponds to a certain pixel is encoded as a color value in the <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Visible_spectrum">visible light spectrum</a>: the points that are closer to the camera would be violet, while the points near the maximum sensor range would be red.
</p><p>NARF also requires us to find suitable keypoints to compute the descriptor for. NARF keypoints are located near an object's corners, and this also requires to find the borders (transitions from foreground to background), which are trivial to find with a range image. Because of this lengthy pipeline, I will describe the whole process in different sections.
</p>
<h3><span class="mw-headline" id="Obtaining_a_range_image">Obtaining a range image</span></h3>
<p>Because we always work with point clouds, I will now explain how you can convert one into a range image, in order to use it for the NARF descriptor. PCL provides a couple of handy classes to perform the conversion, given that you fill the camera data correctly.
</p><p>A range image can be created in two ways. First, we can use spherical projection, which would give us an image similar to the ones produced by a LIDAR sensor. Second, we can use planar projection, which is better suitable for camera-like sensors as the Kinect or the Xtion, and will not have the characteristic distortion of the first one.
</p>
<h4><span class="mw-headline" id="Spherical_projection">Spherical projection</span></h4>
<p>The following code will take a point cloud and create a range image from it, using spherical projection:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/range_image/range_image.h&gt;</span>
<span class="co2">#include &lt;pcl/visualization/range_image_visualizer.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Parameters needed by the range image object:</span>
&nbsp;
	<span class="co1">// Angular resolution is the angular distance between pixels.</span>
	<span class="co1">// Kinect: 57° horizontal FOV, 43° vertical FOV, 640x480 (chosen here).</span>
	<span class="co1">// Xtion: 58° horizontal FOV, 45° vertical FOV, 640x480.</span>
	<span class="kw4">float</span> angularResolutionX <span class="sy1">=</span> <span class="br0">(</span><span class="kw4">float</span><span class="br0">)</span><span class="br0">(</span>57.0f <span class="sy2">/</span> 640.0f <span class="sy2">*</span> <span class="br0">(</span>M_PI <span class="sy2">/</span> 180.0f<span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> angularResolutionY <span class="sy1">=</span> <span class="br0">(</span><span class="kw4">float</span><span class="br0">)</span><span class="br0">(</span>43.0f <span class="sy2">/</span> 480.0f <span class="sy2">*</span> <span class="br0">(</span>M_PI <span class="sy2">/</span> 180.0f<span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Maximum horizontal and vertical angles. For example, for a full panoramic scan,</span>
	<span class="co1">// the first would be 360º. Choosing values that adjust to the real sensor will</span>
	<span class="co1">// decrease the time it takes, but don't worry. If the values are bigger than</span>
	<span class="co1">// the real ones, the image will be automatically cropped to discard empty zones.</span>
	<span class="kw4">float</span> maxAngleX <span class="sy1">=</span> <span class="br0">(</span><span class="kw4">float</span><span class="br0">)</span><span class="br0">(</span>60.0f <span class="sy2">*</span> <span class="br0">(</span>M_PI <span class="sy2">/</span> 180.0f<span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> maxAngleY <span class="sy1">=</span> <span class="br0">(</span><span class="kw4">float</span><span class="br0">)</span><span class="br0">(</span>50.0f <span class="sy2">*</span> <span class="br0">(</span>M_PI <span class="sy2">/</span> 180.0f<span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Sensor pose. Thankfully, the cloud includes the data.</span>
	Eigen<span class="sy4">::</span><span class="me2">Affine3f</span> sensorPose <span class="sy1">=</span> Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>Eigen<span class="sy4">::</span><span class="me2">Translation3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>0<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>1<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>2<span class="br0">]</span><span class="br0">)</span><span class="br0">)</span> <span class="sy2">*</span>
								 Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_orientation_<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Noise level. If greater than 0, values of neighboring points will be averaged.</span>
	<span class="co1">// This would set the search radius (e.g., 0.03 == 3cm).</span>
	<span class="kw4">float</span> noiseLevel <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	<span class="co1">// Minimum range. If set, any point closer to the sensor than this will be ignored.</span>
	<span class="kw4">float</span> minimumRange <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	<span class="co1">// Border size. If greater than 0, a border of "unobserved" points will be left</span>
	<span class="co1">// in the image when it is cropped.</span>
	<span class="kw4">int</span> borderSize <span class="sy1">=</span> <span class="nu0">1</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Range image object.</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImage</span> rangeImage<span class="sy4">;</span>
	rangeImage.<span class="me1">createFromPointCloud</span><span class="br0">(</span><span class="sy2">*</span>cloud, angularResolutionX, angularResolutionY,
									maxAngleX, maxAngleY, sensorPose, pcl<span class="sy4">::</span><span class="me2">RangeImage</span><span class="sy4">::</span><span class="me2">CAMERA_FRAME</span>,
									noiseLevel, minimumRange, borderSize<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Visualize the image.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">RangeImageVisualizer</span> viewer<span class="br0">(</span><span class="st0">"Range image"</span><span class="br0">)</span><span class="sy4">;</span>
	viewer.<span class="me1">showRangeImage</span><span class="br0">(</span>rangeImage<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw1">while</span> <span class="br0">(</span><span class="sy3">!</span>viewer.<span class="me1">wasStopped</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
	<span class="br0">{</span>
		viewer.<span class="me1">spinOnce</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
		<span class="co1">// Sleep 100ms to go easy on the CPU.</span>
		pcl_sleep<span class="br0">(</span>0.1<span class="br0">)</span><span class="sy4">;</span>
	<span class="br0">}</span>
<span class="br0">}</span></pre></div></div>
<p>Here you can see an example of the output range image:
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_spherical.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_spherical.png" width="400" height="323" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_spherical.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Range image of a point cloud, using spherical projection.</div></div></div></div>
<p><br>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Angular resolution, Maximum angle, Sensor pose, Coordinate frame, Noise level, Maximum range, Border size
</li>
<li> <b>Output</b>: Range image (spherical projection)
</li>
<li> <b>Tutorials</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/range_image_creation.php">How to create a range image from a point cloud</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/range_image_visualization.php">How to visualize a range image</a>
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_range_image.html">pcl::RangeImage</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_range_image_spherical.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h4><span class="mw-headline" id="Planar_projection">Planar projection</span></h4>
<p>As mentioned, planar projection will give better results with clouds taken from a depth camera:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/range_image/range_image_planar.h&gt;</span>
<span class="co2">#include &lt;pcl/visualization/range_image_visualizer.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Parameters needed by the planar range image object:</span>
&nbsp;
	<span class="co1">// Image size. Both Kinect and Xtion work at 640x480.</span>
	<span class="kw4">int</span> imageSizeX <span class="sy1">=</span> <span class="nu0">640</span><span class="sy4">;</span>
	<span class="kw4">int</span> imageSizeY <span class="sy1">=</span> <span class="nu0">480</span><span class="sy4">;</span>
	<span class="co1">// Center of projection. here, we choose the middle of the image.</span>
	<span class="kw4">float</span> centerX <span class="sy1">=</span> 640.0f <span class="sy2">/</span> <span class="nu17">2.0f</span><span class="sy4">;</span>
	<span class="kw4">float</span> centerY <span class="sy1">=</span> 480.0f <span class="sy2">/</span> <span class="nu17">2.0f</span><span class="sy4">;</span>
	<span class="co1">// Focal length. The value seen here has been taken from the original depth images.</span>
	<span class="co1">// It is safe to use the same value vertically and horizontally.</span>
	<span class="kw4">float</span> focalLengthX <span class="sy1">=</span> 525.0f, focalLengthY <span class="sy1">=</span> focalLengthX<span class="sy4">;</span>
	<span class="co1">// Sensor pose. Thankfully, the cloud includes the data.</span>
	Eigen<span class="sy4">::</span><span class="me2">Affine3f</span> sensorPose <span class="sy1">=</span> Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>Eigen<span class="sy4">::</span><span class="me2">Translation3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>0<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>1<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>2<span class="br0">]</span><span class="br0">)</span><span class="br0">)</span> <span class="sy2">*</span>
								 Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_orientation_<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Noise level. If greater than 0, values of neighboring points will be averaged.</span>
	<span class="co1">// This would set the search radius (e.g., 0.03 == 3cm).</span>
	<span class="kw4">float</span> noiseLevel <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	<span class="co1">// Minimum range. If set, any point closer to the sensor than this will be ignored.</span>
	<span class="kw4">float</span> minimumRange <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Planar range image object.</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImagePlanar</span> rangeImagePlanar<span class="sy4">;</span>
	rangeImagePlanar.<span class="me1">createFromPointCloudWithFixedSize</span><span class="br0">(</span><span class="sy2">*</span>cloud, imageSizeX, imageSizeY,
			centerX, centerY, focalLengthX, focalLengthX,
			sensorPose, pcl<span class="sy4">::</span><span class="me2">RangeImage</span><span class="sy4">::</span><span class="me2">CAMERA_FRAME</span>,
			noiseLevel, minimumRange<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Visualize the image.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">RangeImageVisualizer</span> viewer<span class="br0">(</span><span class="st0">"Planar range image"</span><span class="br0">)</span><span class="sy4">;</span>
	viewer.<span class="me1">showRangeImage</span><span class="br0">(</span>rangeImagePlanar<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw1">while</span> <span class="br0">(</span><span class="sy3">!</span>viewer.<span class="me1">wasStopped</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
	<span class="br0">{</span>
		viewer.<span class="me1">spinOnce</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
		<span class="co1">// Sleep 100ms to go easy on the CPU.</span>
		pcl_sleep<span class="br0">(</span>0.1<span class="br0">)</span><span class="sy4">;</span>
	<span class="br0">}</span>
<span class="br0">}</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_planar.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_planar.png" width="400" height="300" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_planar.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Range image of a point cloud, using planar projection.</div></div></div></div>
<p><br>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Image size, Projection center, Focal length, Sensor pose, Coordinate frame, Noise level, Maximum range
</li>
<li> <b>Output</b>: Range image (planar projection)
</li>
<li> <b>Tutorials</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/range_image_creation.php">How to create a range image from a point cloud</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/range_image_visualization.php">How to visualize a range image</a>
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_range_image_planar.html">pcl::RangeImagePlanar</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_range_image_planar.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
If you prefer to do the conversion in real time while you inspect the cloud, PCL ships with an <a rel="nofollow" class="external text" href="https://github.com/PointCloudLibrary/pcl/tree/master/doc/tutorials/content/sources/openni_range_image_visualization">example</a> that fetches an <span style="color:#FF1493">"openni_wrapper::DepthImage"</span> from an OpenNI device and creates the range image from it. You can adapt the code of the <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_1:_Installing_and_testing#Testing" title="PCL/OpenNI tutorial 1: Installing and testing"> example</a> example from tutorial 1 to save it to disk with the function <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/group__io.html#ga7291a029cdcde32ca3639d07dc6491b9">pcl::io::saveRangeImagePlanarFilePNG()</a>.
</p>
<h3><span class="mw-headline" id="Extracting_borders">Extracting borders</span></h3>
<p>NARF keypoints are located near the edges of objects in the range image, so in order to find them, we first have to extract the borders. A border is defined as an abrupt change from foreground to background. In a range image, this can be easily seen because there is a "jump" in the depth value of two adjacent pixels.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_border_detection.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_border_detection.png" width="400" height="277" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_border_detection.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Border detection on a range image (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original paper</a>).</div></div></div></div>
<p><br>
There are three types of borders. <i>Object borders</i> consist of the pixels (or points) located on the very edge of an object (the outermost points still belonging to the object). <i>Shadow borders</i> are points in the background on the edge of occlusions (empty areas in the background due to the objects in front covering them). Notice that, when the cloud is seen from the sensor's viewpoint, object and shadow points will seem adjacent. Finally, <i>veil points</i> are points interpolated between the previous two which appear in scans taken with LIDAR sensors, so we do not have to worry about them here.
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_border_type.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_border_type.png" width="135" height="120"></a></div></div>
			<div class="gallerytext">
<p>Types of borders (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original paper</a>).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_borders_example.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_borders_example.png" width="167" height="120"></a></div></div>
			<div class="gallerytext">
<p>Example of object and shadow borders on a cloud.
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
The algorithm basically compares a point's depth with the values of its neighbors, and if a big difference is found, we know it is due to a border. Points closer to the sensor will be marked as object borders, and the other ones as shadow borders.
</p><p>PCL provides a class for extracting borders of a range image:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/range_image/range_image_planar.h&gt;</span>
<span class="co2">#include &lt;pcl/features/range_image_border_extractor.h&gt;</span>
<span class="co2">#include &lt;pcl/visualization/range_image_visualizer.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the borders.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">BorderDescription</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> borders<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">BorderDescription</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Convert the cloud to range image.</span>
	<span class="kw4">int</span> imageSizeX <span class="sy1">=</span> 640, imageSizeY <span class="sy1">=</span> <span class="nu0">480</span><span class="sy4">;</span>
	<span class="kw4">float</span> centerX <span class="sy1">=</span> <span class="br0">(</span>640.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span>, centerY <span class="sy1">=</span> <span class="br0">(</span>480.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> focalLengthX <span class="sy1">=</span> 525.0f, focalLengthY <span class="sy1">=</span> focalLengthX<span class="sy4">;</span>
	Eigen<span class="sy4">::</span><span class="me2">Affine3f</span> sensorPose <span class="sy1">=</span> Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>Eigen<span class="sy4">::</span><span class="me2">Translation3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>0<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>1<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>2<span class="br0">]</span><span class="br0">)</span><span class="br0">)</span> <span class="sy2">*</span>
								 Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_orientation_<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> noiseLevel <span class="sy1">=</span> 0.0f, minimumRange <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImagePlanar</span> rangeImage<span class="sy4">;</span>
	rangeImage.<span class="me1">createFromPointCloudWithFixedSize</span><span class="br0">(</span><span class="sy2">*</span>cloud, imageSizeX, imageSizeY,
			centerX, centerY, focalLengthX, focalLengthX,
			sensorPose, pcl<span class="sy4">::</span><span class="me2">RangeImage</span><span class="sy4">::</span><span class="me2">CAMERA_FRAME</span>,
			noiseLevel, minimumRange<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Border extractor object.</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImageBorderExtractor</span> borderExtractor<span class="br0">(</span><span class="sy3">&amp;</span>rangeImage<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	borderExtractor.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>borders<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Visualize the borders.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">RangeImageVisualizer</span><span class="sy2">*</span> viewer <span class="sy1">=</span> <span class="kw2">NULL</span><span class="sy4">;</span>
	viewer <span class="sy1">=</span> pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">RangeImageVisualizer</span><span class="sy4">::</span><span class="me2">getRangeImageBordersWidget</span><span class="br0">(</span>rangeImage,
			 <span class="sy2">-</span>std<span class="sy4">::</span><span class="me2">numeric_limits</span><span class="sy1">&lt;</span><span class="kw4">float</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">infinity</span><span class="br0">(</span><span class="br0">)</span>,
			 std<span class="sy4">::</span><span class="me2">numeric_limits</span><span class="sy1">&lt;</span><span class="kw4">float</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">infinity</span><span class="br0">(</span><span class="br0">)</span>,
			 <span class="kw2">false</span>, <span class="sy2">*</span>borders, <span class="st0">"Borders"</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="kw1">while</span> <span class="br0">(</span><span class="sy3">!</span>viewer<span class="sy2">-</span><span class="sy1">&gt;</span>wasStopped<span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
	<span class="br0">{</span>
		viewer<span class="sy2">-</span><span class="sy1">&gt;</span>spinOnce<span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
		<span class="co1">// Sleep 100ms to go easy on the CPU.</span>
		pcl_sleep<span class="br0">(</span>0.1<span class="br0">)</span><span class="sy4">;</span>
	<span class="br0">}</span>
<span class="br0">}</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_borders.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_borders.png" width="400" height="300" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_borders.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Borders found on the range image.</div></div></div></div>
<p><br>
You can use the extractor's <span style="color:#FF1493">"getParameters()"</span> function to get a <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/structpcl_1_1_range_image_border_extractor_1_1_parameters.html">pcl::RangeImageBorderExtractor::Parameters</a> struct with the settings that will be used.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Range image
</li>
<li> <b>Output</b>: Borders
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/range_image_border_extraction.php">How to extract borders from range images</a>
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://europa.informatik.uni-freiburg.de/files/steder11icra.pdf">Point Feature Extraction on 3D Range Scans Taking into Account Object Boundaries</a> (Bastian Steder et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_range_image_border_extractor.html">pcl::RangeImageBorderExtractor</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_range_image_borders.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h3><span class="mw-headline" id="Finding_keypoints">Finding keypoints</span></h3>
<p>Citing the <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original publication</a>:
</p><p><i>"We have the following requirements for our interest point extraction procedure:</i>
</p>
<ol>
<li> <i>It must take information about borders and the surface structure into account.</i>
</li>
<li> <i>It must select positions that can be reliably detected even if the object is observed from another perspective.</i>
</li>
<li> <i>The points must be on positions that provide stable areas for normal estimation or the descriptor calculation in general."</i>
</li>
</ol>
<p>The procedure is the following: for every point in the range image, a score is computed that conveys how much the surface changes in its neighborhood (this is tuned with the <i>support size</i> σ, which is the diameter of the sphere used to find neighboring points). Also, the dominant direction of this change is computed. Then, this direction is compared with those of the neighbors, trying to find how stable the point is (if the directions are very different, that means the point is not stable, and that the surface around changes a lot). Points that are near the object's corners (but not exactly on the very edge) will be good keypoints, yet stable enough.
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:NARF_keypoints.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/NARF_keypoints.png" width="116" height="120"></a></div></div>
			<div class="gallerytext">
<p>NARF keypoints (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original paper</a>).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:NARF_keypoints_support_sizes.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/NARF_keypoints_support_sizes.png" width="179" height="120"></a></div></div>
			<div class="gallerytext">
<p>Interest regions with a support size of 20cm (up) and 1m (down) (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original paper</a>).
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
In PCL, NARF keypoints can be found this way:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/range_image/range_image_planar.h&gt;</span>
<span class="co2">#include &lt;pcl/features/range_image_border_extractor.h&gt;</span>
<span class="co2">#include &lt;pcl/keypoints/narf_keypoint.h&gt;</span>
<span class="co2">#include &lt;pcl/visualization/range_image_visualizer.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the keypoints' indices.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span><span class="kw4">int</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> keypoints<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span><span class="kw4">int</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Convert the cloud to range image.</span>
	<span class="kw4">int</span> imageSizeX <span class="sy1">=</span> 640, imageSizeY <span class="sy1">=</span> <span class="nu0">480</span><span class="sy4">;</span>
	<span class="kw4">float</span> centerX <span class="sy1">=</span> <span class="br0">(</span>640.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span>, centerY <span class="sy1">=</span> <span class="br0">(</span>480.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> focalLengthX <span class="sy1">=</span> 525.0f, focalLengthY <span class="sy1">=</span> focalLengthX<span class="sy4">;</span>
	Eigen<span class="sy4">::</span><span class="me2">Affine3f</span> sensorPose <span class="sy1">=</span> Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>Eigen<span class="sy4">::</span><span class="me2">Translation3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>0<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>1<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>2<span class="br0">]</span><span class="br0">)</span><span class="br0">)</span> <span class="sy2">*</span>
								 Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_orientation_<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> noiseLevel <span class="sy1">=</span> 0.0f, minimumRange <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImagePlanar</span> rangeImage<span class="sy4">;</span>
	rangeImage.<span class="me1">createFromPointCloudWithFixedSize</span><span class="br0">(</span><span class="sy2">*</span>cloud, imageSizeX, imageSizeY,
			centerX, centerY, focalLengthX, focalLengthX,
			sensorPose, pcl<span class="sy4">::</span><span class="me2">RangeImage</span><span class="sy4">::</span><span class="me2">CAMERA_FRAME</span>,
			noiseLevel, minimumRange<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	pcl<span class="sy4">::</span><span class="me2">RangeImageBorderExtractor</span> borderExtractor<span class="sy4">;</span>
	<span class="co1">// Keypoint detection object.</span>
	pcl<span class="sy4">::</span><span class="me2">NarfKeypoint</span> detector<span class="br0">(</span><span class="sy3">&amp;</span>borderExtractor<span class="br0">)</span><span class="sy4">;</span>
	detector.<span class="me1">setRangeImage</span><span class="br0">(</span><span class="sy3">&amp;</span>rangeImage<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// The support size influences how big the surface of interest will be,</span>
	<span class="co1">// when finding keypoints from the border information.</span>
	detector.<span class="me1">getParameters</span><span class="br0">(</span><span class="br0">)</span>.<span class="me1">support_size</span> <span class="sy1">=</span> <span class="nu17">0.2f</span><span class="sy4">;</span>
&nbsp;
	detector.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>keypoints<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Visualize the keypoints.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">RangeImageVisualizer</span> viewer<span class="br0">(</span><span class="st0">"NARF keypoints"</span><span class="br0">)</span><span class="sy4">;</span>
	viewer.<span class="me1">showRangeImage</span><span class="br0">(</span>rangeImage<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw1">for</span> <span class="br0">(</span><span class="kw4">size_t</span> i <span class="sy1">=</span> <span class="nu0">0</span><span class="sy4">;</span> i <span class="sy1">&lt;</span> keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>points.<span class="me1">size</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span> <span class="sy2">++</span>i<span class="br0">)</span>
	<span class="br0">{</span>
		viewer.<span class="me1">markPoint</span><span class="br0">(</span>keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>points<span class="br0">[</span>i<span class="br0">]</span> <span class="sy2">%</span> rangeImage.<span class="me1">width</span>,
						 keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>points<span class="br0">[</span>i<span class="br0">]</span> <span class="sy2">/</span> rangeImage.<span class="me1">width</span>,
						 <span class="co1">// Set the color of the pixel to red (the background</span>
						 <span class="co1">// circle is already that color). All other parameters</span>
						 <span class="co1">// are left untouched, check the API for more options.</span>
						 pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">Vector3ub</span><span class="br0">(</span>1.0f, 0.0f, 0.0f<span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="kw1">while</span> <span class="br0">(</span><span class="sy3">!</span>viewer.<span class="me1">wasStopped</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span>
	<span class="br0">{</span>
		viewer.<span class="me1">spinOnce</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
		<span class="co1">// Sleep 100ms to go easy on the CPU.</span>
		pcl_sleep<span class="br0">(</span>0.1<span class="br0">)</span><span class="sy4">;</span>
	<span class="br0">}</span>
<span class="br0">}</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_NARF_keypoints.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Range_image_NARF_keypoints.png" width="400" height="300" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Range_image_NARF_keypoints.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>NARF keypoints found on the range image.</div></div></div></div>
<p><br>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Range image, Border extractor, Support Size
</li>
<li> <b>Output</b>: NARF keypoints
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/narf_keypoint_extraction.php">How to extract NARF keypoint from a range image</a>
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://europa.informatik.uni-freiburg.de/files/steder11icra.pdf">Point Feature Extraction on 3D Range Scans Taking into Account Object Boundaries</a> (Bastian Steder et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_narf_keypoint.html">pcl::NarfKeypoint</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_NARF_keypoints.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h3><span class="mw-headline" id="Computing_the_descriptor">Computing the descriptor</span></h3>
<p>We have created the range image from a point cloud, and we have extracted the borders in order to find good keypoints. Now it is time to compute the NARF descriptor for each keypoint.
</p><p>The NARF descriptor encodes information about surface changes around a point. First, a local range patch is created around the point. It is like a small range image centered at that point, aligned with the normal (it would seem as if we were looking at the point along the normal). Then, a star pattern with <i>n</i> beams is overlaid onto the patch, also centered at the point. For every beam, a value is computed, that reflects how much the surface under it changes. The stronger the change is, and the closer to the center it is, the higher the final value will be. The <i>n</i> resulting values compose the final descriptor.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:602px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:NARF_descriptor.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/NARF_descriptor.png" width="600" height="242" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:NARF_descriptor.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Computing the NARF descriptor for a keypoint (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/icra2011_3dfeatures.pdf">original paper</a>).</div></div></div></div>
<p><br>
The descriptor right now is not invariant to rotations around the normal. To achieve this, the whole possible 360 degrees are binned into an histogram. The value of each bin is computed from the descriptor values according to the angle. Then, the bin with the highest value is considered the dominant orientation, and the descriptor is shifted according to it.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/range_image/range_image_planar.h&gt;</span>
<span class="co2">#include &lt;pcl/features/range_image_border_extractor.h&gt;</span>
<span class="co2">#include &lt;pcl/keypoints/narf_keypoint.h&gt;</span>
<span class="co2">#include &lt;pcl/features/narf_descriptor.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the keypoints' indices.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span><span class="kw4">int</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> keypoints<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span><span class="kw4">int</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the NARF descriptors.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Narf36</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Narf36</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Convert the cloud to range image.</span>
	<span class="kw4">int</span> imageSizeX <span class="sy1">=</span> 640, imageSizeY <span class="sy1">=</span> <span class="nu0">480</span><span class="sy4">;</span>
	<span class="kw4">float</span> centerX <span class="sy1">=</span> <span class="br0">(</span>640.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span>, centerY <span class="sy1">=</span> <span class="br0">(</span>480.0f <span class="sy2">/</span> 2.0f<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> focalLengthX <span class="sy1">=</span> 525.0f, focalLengthY <span class="sy1">=</span> focalLengthX<span class="sy4">;</span>
	Eigen<span class="sy4">::</span><span class="me2">Affine3f</span> sensorPose <span class="sy1">=</span> Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>Eigen<span class="sy4">::</span><span class="me2">Translation3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>0<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>1<span class="br0">]</span>,
								 cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_origin_<span class="br0">[</span>2<span class="br0">]</span><span class="br0">)</span><span class="br0">)</span> <span class="sy2">*</span>
								 Eigen<span class="sy4">::</span><span class="me2">Affine3f</span><span class="br0">(</span>cloud<span class="sy2">-</span><span class="sy1">&gt;</span>sensor_orientation_<span class="br0">)</span><span class="sy4">;</span>
	<span class="kw4">float</span> noiseLevel <span class="sy1">=</span> 0.0f, minimumRange <span class="sy1">=</span> <span class="nu17">0.0f</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImagePlanar</span> rangeImage<span class="sy4">;</span>
	rangeImage.<span class="me1">createFromPointCloudWithFixedSize</span><span class="br0">(</span><span class="sy2">*</span>cloud, imageSizeX, imageSizeY,
			centerX, centerY, focalLengthX, focalLengthX,
			sensorPose, pcl<span class="sy4">::</span><span class="me2">RangeImage</span><span class="sy4">::</span><span class="me2">CAMERA_FRAME</span>,
			noiseLevel, minimumRange<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Extract the keypoints.</span>
	pcl<span class="sy4">::</span><span class="me2">RangeImageBorderExtractor</span> borderExtractor<span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">NarfKeypoint</span> detector<span class="br0">(</span><span class="sy3">&amp;</span>borderExtractor<span class="br0">)</span><span class="sy4">;</span>
	detector.<span class="me1">setRangeImage</span><span class="br0">(</span><span class="sy3">&amp;</span>rangeImage<span class="br0">)</span><span class="sy4">;</span>
	detector.<span class="me1">getParameters</span><span class="br0">(</span><span class="br0">)</span>.<span class="me1">support_size</span> <span class="sy1">=</span> <span class="nu17">0.2f</span><span class="sy4">;</span>
	detector.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>keypoints<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// The NARF estimator needs the indices in a vector, not a cloud.</span>
	std<span class="sy4">::</span><span class="me2">vector</span><span class="sy1">&lt;</span><span class="kw4">int</span><span class="sy1">&gt;</span> keypoints2<span class="sy4">;</span>
	keypoints2.<span class="me1">resize</span><span class="br0">(</span>keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>points.<span class="me1">size</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="kw1">for</span> <span class="br0">(</span><span class="kw4">unsigned</span> <span class="kw4">int</span> i <span class="sy1">=</span> <span class="nu0">0</span><span class="sy4">;</span> i <span class="sy1">&lt;</span> keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>size<span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span> <span class="sy2">++</span>i<span class="br0">)</span>
		keypoints2<span class="br0">[</span>i<span class="br0">]</span> <span class="sy1">=</span> keypoints<span class="sy2">-</span><span class="sy1">&gt;</span>points<span class="br0">[</span>i<span class="br0">]</span><span class="sy4">;</span>
	<span class="co1">// NARF estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">NarfDescriptor</span> narf<span class="br0">(</span><span class="sy3">&amp;</span>rangeImage, <span class="sy3">&amp;</span>keypoints2<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Support size: choose the same value you used for keypoint extraction.</span>
	narf.<span class="me1">getParameters</span><span class="br0">(</span><span class="br0">)</span>.<span class="me1">support_size</span> <span class="sy1">=</span> <span class="nu17">0.2f</span><span class="sy4">;</span>
	<span class="co1">// If true, the rotation invariant version of NARF will be used. The histogram</span>
	<span class="co1">// will be shifted according to the dominant orientation to provide robustness to</span>
	<span class="co1">// rotations around the normal.</span>
	narf.<span class="me1">getParameters</span><span class="br0">(</span><span class="br0">)</span>.<span class="me1">rotation_invariant</span> <span class="sy1">=</span> <span class="kw2">true</span><span class="sy4">;</span>
&nbsp;
	narf.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Range image, Key points, Support Size
</li>
<li> <b>Output</b>: NARF descriptors
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/narf_feature_extraction.php">How to extract NARF Features from a range image</a>
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://europa.informatik.uni-freiburg.de/files/steder11icra.pdf">Point Feature Extraction on 3D Range Scans Taking into Account Object Boundaries</a> (Bastian Steder et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_narf_descriptor.html">pcl::NarfDescriptor</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_NARF_descriptor.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="RoPS">RoPS</span></h2>
<p>The Rotational Projection Statistics (RoPS) feature is a bit different from the other descriptors because it works with a triangle mesh, so a previous triangulation step is needed for generating this mesh from the cloud. Apart from that, most concepts are similar.
</p><p>In order to compute RoPS for a keypoint, the local surface is cropped according to a support radius, so only points and triangles lying inside are taken into account. Then, a local reference frame (LRF) is computed, giving the descriptor its rotational invariance. A coordinate system is created with the point as the origin, and the axes aligned with the LRF. Then, for every axis, several steps are performed.
</p><p>First, the local surface is rotated around the current axis. The angle is determined by one of the parameters, which sets the number of rotations. Then, all points in the local surface are projected onto the XY, XZ and YZ planes. For each one, statistical information about the distribution of the projected points is computed, and concatenated to form the final descriptor.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/point_types_conversion.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/surface/gp3.h&gt;</span>
<span class="co2">#include &lt;pcl/features/rops_estimation.h&gt;</span>
&nbsp;
<span class="co1">// A handy typedef.</span>
<span class="kw4">typedef</span> pcl<span class="sy4">::</span><span class="me2">Histogram</span><span class="sy1">&lt;</span>135<span class="sy1">&gt;</span> ROPS135<span class="sy4">;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing both the points and the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointNormal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloudNormals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointNormal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the ROPS descriptor for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>ROPS135<span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>ROPS135<span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Perform triangulation.</span>
	pcl<span class="sy4">::</span><span class="me2">concatenateFields</span><span class="br0">(</span><span class="sy2">*</span>cloud, <span class="sy2">*</span>normals, <span class="sy2">*</span>cloudNormals<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointNormal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree2<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointNormal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	kdtree2<span class="sy2">-</span><span class="sy1">&gt;</span>setInputCloud<span class="br0">(</span>cloudNormals<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">GreedyProjectionTriangulation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointNormal</span><span class="sy1">&gt;</span> triangulation<span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">PolygonMesh</span> triangles<span class="sy4">;</span>
	triangulation.<span class="me1">setSearchRadius</span><span class="br0">(</span>0.025<span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">setMu</span><span class="br0">(</span>2.5<span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">setMaximumNearestNeighbors</span><span class="br0">(</span>100<span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">setMaximumSurfaceAngle</span><span class="br0">(</span>M_PI <span class="sy2">/</span> 4<span class="br0">)</span><span class="sy4">;</span> <span class="co1">// 45 degrees.</span>
	triangulation.<span class="me1">setNormalConsistency</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">setMinimumAngle</span><span class="br0">(</span>M_PI <span class="sy2">/</span> 18<span class="br0">)</span><span class="sy4">;</span> <span class="co1">// 10 degrees.</span>
	triangulation.<span class="me1">setMaximumAngle</span><span class="br0">(</span>2 <span class="sy2">*</span> M_PI <span class="sy2">/</span> 3<span class="br0">)</span><span class="sy4">;</span> <span class="co1">// 120 degrees.</span>
	triangulation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloudNormals<span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree2<span class="br0">)</span><span class="sy4">;</span>
	triangulation.<span class="me1">reconstruct</span><span class="br0">(</span>triangles<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should only compute descriptors for chosen keypoints. It has</span>
	<span class="co1">// been omitted here for simplicity.</span>
&nbsp;
	<span class="co1">// RoPs estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">ROPSEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, ROPS135<span class="sy1">&gt;</span> rops<span class="sy4">;</span>
	rops.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	rops.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	rops.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	rops.<span class="me1">setTriangles</span><span class="br0">(</span>triangles.<span class="me1">polygons</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Number of partition bins that is used for distribution matrix calculation.</span>
	rops.<span class="me1">setNumberOfPartitionBins</span><span class="br0">(</span>5<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// The greater the number of rotations is, the bigger the resulting descriptor.</span>
	<span class="co1">// Make sure to change the histogram size accordingly.</span>
	rops.<span class="me1">setNumberOfRotations</span><span class="br0">(</span>3<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Support radius that is used to crop the local surface of the point.</span>
	rops.<span class="me1">setSupportRadius</span><span class="br0">(</span>0.025<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	rops.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points, Triangles, Search method, Support radius, Number of rotations, Number of partition bins
</li>
<li> <b>Output</b>: RoPS descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://arxiv.org/pdf/1304.3192v1.pdf">Rotational Projection Statistics for 3D Local Surface Description and Object Recognition</a> (Yulan Guo et al., 2013)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_r_o_p_s_estimation.html">pcl::ROPSEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_RoPS.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h1><span class="mw-headline" id="Global_descriptors">Global descriptors</span></h1>
<p>Global descriptors encode object geometry. They are not computed for individual points, but for a whole cluster that represents an object. Because of this, a preprocessing step (<a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_3:_Cloud_processing_(advanced)#Segmentation" title="PCL/OpenNI tutorial 3: Cloud processing (advanced)">segmentation</a>) is required, in order to retrieve possible candidates.
</p><p>Global descriptors are used for object recognition and classification, geometric analysis (object type, shape...), and pose estimation.
</p><p>You should also know that many local descriptors can also be used as global ones. This can be done with descriptors that use a radius to search for neighbors (as PFH does). The trick is to compute it for one single point in the object cluster, and set the radius to the maximum possible distance between any two points (so all points in the cluster are considered as neighbors).
</p>
<h2><span class="mw-headline" id="VFH">VFH</span></h2>
<p>The Viewpoint Feature Histogram is based on the FPFH. Because the latter is invariant to the object's pose, the authors decided to expand it by including information about the viewpoint. Also, the FPFH is estimated once for the whole cluster, not for every point.
</p><p>The VFH is made up by two parts: a viewpoint direction component, and an extended FPFH component. To compute the first one, the object's <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_2:_Cloud_processing_(basic)#Computing_the_centroid" title="PCL/OpenNI tutorial 2: Cloud processing (basic)">centroid</a> is found, which is the point that results from averaging the X, Y and Z coordinates of all points. Then, the vector between the viewpoint (the position of the sensor) and this centroid is computed, and normalized. Finally, for all points in the cluster, the angle between this vector and their normal is calculated, and the result is binned into an histogram. The vector is translated to each point when computing the angle because it makes the descriptor scale invariant.
</p><p>The second component is computed like the FPFH (that results in 3 histograms for the 3 angular features, α, φ and θ), with some differences: it is only computed for the centroid, using the computed viewpoint direction vector as its normal (as the point, obviously, does not have a normal), and setting all the cluster's points as neighbors.
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:VFH_viewpoint_component.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/VFH_viewpoint_component.png" width="241" height="120"></a></div></div>
			<div class="gallerytext">
<p>Viewpoint component of the VFH (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/Rusu10IROS.pdf">original paper</a>).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:VFH_extended_FPFH_component.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/VFH_extended_FPFH_component.png" width="179" height="120"></a></div></div>
			<div class="gallerytext">
<p>Extended FPFH component of the VFH (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/Rusu10IROS.pdf">original paper</a>).
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
The resulting 4 histograms (1 for the viewpoint component, 3 for the extended FPFH component) are concatenated to build the final VFH descriptor. By default, the bins are normalized using the total number of points in the cluster. This makes the VFH descriptor invariant to scale.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:VFH_histogram.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/VFH_histogram.png" width="400" height="255" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:VFH_histogram.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>VFH histogram (image from <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/Rusu10IROS.pdf">original paper</a>).</div></div></div></div>
<p><br>
The PCL implementation computes an additional fifth histogram with the distances of the cluster points to the centroid (the Shape Distribution Component, SDC), increading the size of the output descriptor from 263 to 308. The SDC is taken from the CVFH descriptor that we will see in the next section, and makes the result more robust.
</p><p>The VFH of an already clustered object can be computed this way:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/vfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Cloud for storing the object.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the VFH descriptor.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should have performed preprocessing to cluster out the object</span>
	<span class="co1">// from the cloud, and save it to this individual file.</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// VFH estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">VFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span> vfh<span class="sy4">;</span>
	vfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Optionally, we can normalize the bins of the resulting histogram,</span>
	<span class="co1">// using the total number of points.</span>
	vfh.<span class="me1">setNormalizeBins</span><span class="br0">(</span><span class="kw2">true</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Also, we can normalize the SDC with the maximum size found between</span>
	<span class="co1">// the centroid and any of the cluster's points.</span>
	vfh.<span class="me1">setNormalizeDistance</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	vfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptor<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>Because only one VFH descriptor is computed for the whole cluster, the size of the cloud object that stores the result will be 1.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster), Normals, Search method, [Normalize bins], [Normalize SDC]
</li>
<li> <b>Output</b>: VFH descriptor
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/vfh_estimation.php">Estimating VFH signatures for a set of points</a>
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://www.willowgarage.com/sites/default/files/Rusu10IROS.pdf">Fast 3D Recognition and Pose Using the Viewpoint Feature Histogram</a> (Radu Bogdan Rusu et al., 2010)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_v_f_h_estimation.html">pcl::VFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_VFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h3><span class="mw-headline" id="CVFH">CVFH</span></h3>
<p>The original VFH descriptor is not robust to occlusion or other sensor artifacts, or measurement errors. If the object cluster is missing many points, the resulting computed centroid will differ from the original, altering the final descriptor, and preventing a positive match from being found. Because of that, the Clustered Viewpoint Feature Histogram (CVFH) was introduced.
</p><p>The idea is very simple: instead of computing a single VFH histogram for the whole cluster, the object is first divided in stable, smooth regions using <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_3:_Cloud_processing_(advanced)#Region_growing" title="PCL/OpenNI tutorial 3: Cloud processing (advanced)"> region-growing segmentation</a>, that enforces several constraints in the distances and differences of normals of the points belonging to every region. Then, a VFH is computed for every region. Thanks to this, an object can be found in a scene, as long as at least one of its regions is fully visible.
</p><p><br>
</p>
<center><ul class="gallery mw-gallery-traditional">
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:CVFH_occlusion.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/CVFH_occlusion.png" width="174" height="120"></a></div></div>
			<div class="gallerytext">
<p>Typical occlusion issues in a point cloud (image from original paper).
</p>
			</div>
		</div></li>
		<li class="gallerybox" style="width: 335px"><div style="width: 335px">
			<div class="thumb" style="width: 330px;"><div style="margin:15px auto;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:CVFH_regions.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/CVFH_regions.png" width="73" height="120"></a></div></div>
			<div class="gallerytext">
<p>Object regions computed for the CVFH (image from original paper).
</p>
			</div>
		</div></li>
</ul></center>
<p><br>
Additionally, a Shape Distribution Component (SDC) is also computed and included. It encodes information about the distribution of the points arond the region's centroid, measuring the distances. The SDC allows to differentiate objects with similar characteristics (size and normal distribution), like two planar surfaces from each other.
</p><p>The authors proposed to discard the histogram normalization step that is performed in VFH. This has the effect of making the descriptor dependant of scale, so an object of a certain size would not match a bigger or smaller copy of itself. It also makes CVFH more robust to occlusion.
</p><p>CVFH is invariant to the camera roll angle, like most global descriptors. This is so because rotations about that camera axis do not change the observable geometry that descriptors are computed from, limiting the pose estimation to 5 DoF. The use of a Camera Roll Histogram (CRH) has been proposed to overcome this.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/cvfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Cloud for storing the object.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the CVFH descriptors.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should have performed preprocessing to cluster out the object</span>
	<span class="co1">// from the cloud, and save it to this individual file.</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// CVFH estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">CVFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span> cvfh<span class="sy4">;</span>
	cvfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	cvfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	cvfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the maximum allowable deviation of the normals,</span>
	<span class="co1">// for the region segmentation step.</span>
	cvfh.<span class="me1">setEPSAngleThreshold</span><span class="br0">(</span>5.0 <span class="sy2">/</span> 180.0 <span class="sy2">*</span> M_PI<span class="br0">)</span><span class="sy4">;</span> <span class="co1">// 5 degrees.</span>
	<span class="co1">// Set the curvature threshold (maximum disparity between curvatures),</span>
	<span class="co1">// for the region segmentation step.</span>
	cvfh.<span class="me1">setCurvatureThreshold</span><span class="br0">(</span>1.0<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set to true to normalize the bins of the resulting histogram,</span>
	<span class="co1">// using the total number of points. Note: enabling it will make CVFH</span>
	<span class="co1">// invariant to scale just like VFH, but the authors encourage the opposite.</span>
	cvfh.<span class="me1">setNormalizeBins</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	cvfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>You can further customize the segmentation step with <span style="color:#FF1493">"setClusterTolerance()"</span> (to set the maximum Euclidean distance between points in the same cluster) and <span style="color:#FF1493">"setMinPoints()"</span>. The size of the output will be equal to the number of regions the object was divided in. Also, check the functions <span style="color:#FF1493">"getCentroidClusters()"</span> and <span style="color:#FF1493">"getCentroidNormalClusters()"</span>, you can use them to get information about the centroids used to compute the different CVFH descriptors.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster), Normals, Search method, Angle threshold, Curvature threshold, [Normalize bins], [Cluster tolerance], [Minimum points]
</li>
<li> <b>Output</b>: CVFH descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6130296">CAD-Model Recognition and 6DOF Pose Estimation Using 3D Cues</a> (requires IEEE Xplore subscription) (Aitor Aldoma et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_c_v_f_h_estimation.html">pcl::CVFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_CVFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h4><span class="mw-headline" id="OUR-CVFH">OUR-CVFH</span></h4>
<p>The Oriented, Unique and Repeatable CVFH expands the previous descriptor, adding the computation of an unique reference frame to make it more robust.
</p><p>OUR-CVFH relies on the use of Semi-Global Unique Reference Frames (SGURFs), which are repeatable coordinate systems computed for each region. Not only they remove the invariance to camera roll and allow to extract the 6DoF pose directly without additional steps, but they also improve the spatial descriptiveness.
</p><p>The first part of the computation is akin to CVFH, but after segmentation, the points in each region are filtered once more according to the difference between their normals and the region's average normal. This results in better shaped regions, improving the estimation of the Reference Frames (RFs).
</p><p>After this, the SGURF is computed for each region. Disambiguation is performed to decide the sign of the axes, according to the points' distribution. If this is not enough and the sign remains ambiguous, multiple RFs will need to be created to account for it. Finally, the OUR-CVFH descriptor is computed. The original Shape Distribution Component (SDC) is discarded, and the surface is now described according to the RFs.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:602px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:OUR-CVFH.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/OUR-CVFH.png" width="600" height="227" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:OUR-CVFH.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>SGURF frame and resulting histogram of a region (image from <a rel="nofollow" class="external text" href="http://vision.deis.unibo.it/fede/papers/dagm12.pdf">original paper</a>).</div></div></div></div>
<p><br>
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/our_cvfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Cloud for storing the object.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the OUR-CVFH descriptors.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should have performed preprocessing to cluster out the object</span>
	<span class="co1">// from the cloud, and save it to this individual file.</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// OUR-CVFH estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">OURCVFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span> ourcvfh<span class="sy4">;</span>
	ourcvfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	ourcvfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	ourcvfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	ourcvfh.<span class="me1">setEPSAngleThreshold</span><span class="br0">(</span>5.0 <span class="sy2">/</span> 180.0 <span class="sy2">*</span> M_PI<span class="br0">)</span><span class="sy4">;</span> <span class="co1">// 5 degrees.</span>
	ourcvfh.<span class="me1">setCurvatureThreshold</span><span class="br0">(</span>1.0<span class="br0">)</span><span class="sy4">;</span>
	ourcvfh.<span class="me1">setNormalizeBins</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the minimum axis ratio between the SGURF axes. At the disambiguation phase,</span>
	<span class="co1">// this will decide if additional Reference Frames need to be created, if ambiguous.</span>
	ourcvfh.<span class="me1">setAxisRatio</span><span class="br0">(</span>0.8<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	ourcvfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p>You can use the <span style="color:#FF1493">"getTransforms()"</span> function to get the transformations aligning the cloud to the corresponding SGURF.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster), Normals, Search method, Angle threshold, Curvature threshold, [Normalize bins], [Cluster tolerance], [Minimum points], [Axis ratio]
</li>
<li> <b>Output</b>: OUR-CVFH descriptors
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://vision.deis.unibo.it/fede/papers/dagm12.pdf">OUR-CVFH – Oriented, Unique and Repeatable Clustered Viewpoint Feature Histogram for Object Recognition and 6DOF Pose Estimation</a> (Aitor Aldoma et al., 2012)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_o_u_r_c_v_f_h_estimation.html">pcl::OURCVFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_OUR-CVFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="ESF">ESF</span></h2>
<p>The Ensemble of Shape Functions (ESF) is a combination of 3 different shape functions that describe certain properties of the cloud's points: distances, angles and area. This descriptor is very unique because it does not require normal information. Actually, it does not need any preprocessing, as it is robust to noise and incomplete surfaces.
</p><p>The algorithm uses a voxel grid as an approximation of the real surface. It iterates through all the points in the cloud: for every iteration, 3 random points are chosen. For these points, the shape functions are computed:
</p>
<ul>
<li> <b>D2</b>: this function computes the distances between point pairs (3 overall). Then, for every pair, it checks if the line that connects both points lies entirely inside the surface, entirely outside (crossing free space), or both. Depending on this, the distance value will be binned to one of three possible histograms: IN, OUT or MIXED.
</li>
<li> <b>D2 ratio</b>: an additional histogram for the ratio between parts of the line inside the surface, and parts outside. This value will be 0 if the line is completely outside, 1 if completely inside, and some value in between if mixed.
</li>
<li> <b>D3</b>: this computes the square root of the area of the triangle formed by the 3 points. Like D2, the result is also classified as IN, OUT or MIXED, each with its own histogram.
</li>
<li> <b>A3</b>: this function computes the angle formed by the points. Then, the value is binned depending on how the line opposite to the angle is (once again, as IN, OUT or MIXED).
</li>
</ul>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:602px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:ESF.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/ESF.png" width="600" height="109" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:ESF.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>ESF descriptor (image from <a rel="nofollow" class="external text" href="http://www.inf.ethz.ch/personal/zeislb/publications/aldoma_2012jram_PCLTutorial.pdf">this paper</a>).</div></div></div></div>
<p><br>
After the loop is over, we are left with 10 subhistograms (IN, OUT and MIXED for D2, D3 and A3, and an additional one for the ratio). Each one has 64 bins, so the size of the final ESF descriptor is 640.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/esf.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Cloud for storing the object.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the ESF descriptor.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">ESFSignature640</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">ESFSignature640</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should have performed preprocessing to cluster out the object</span>
	<span class="co1">// from the cloud, and save it to this individual file.</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// ESF estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">ESFEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">ESFSignature640</span><span class="sy1">&gt;</span> esf<span class="sy4">;</span>
	esf.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	esf.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptor<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster)
</li>
<li> <b>Output</b>: ESF descriptor
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6181760">Ensemble of shape functions for 3D object classification</a> (requires IEEE Xplore subscription) (Walter Wohlkinger and Markus Vincze, 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_e_s_f_estimation.html">pcl::ESFEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_ESF.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="GFPFH">GFPFH</span></h2>
<p>As you may have guessed, GFPFH stands for Global Fast Point Feature Histogram, the global version of the FPFH descriptor. GFPFH was designed for the task of helping a robot navigate its environment, having some context of the objects around it.
</p><p>The first step before being able to compute the descriptor is surface categorization. A set of logical primitives (the classes, or categories) is created, which depends on the type of objects we expect the robot to find on the scene. For example, if we know there will be a coffee mug, we create three: one for the handle, and the other two for the outer and inner faces. Then, FPFH descriptors are computed, and everything is fed to a <a rel="nofollow" class="external text" href="http://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Field</a> (CRF) algorithm. The CRF will label each surface with one of the previous categories, so we end up with a cloud where each point has been classified depending of the type of object (or object's region) it belongs to.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:302px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:FPFH_CRF.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/FPFH_CRF.png" width="300" height="220" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:FPFH_CRF.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Classification of objects made with FPFH and CRF (image from <a rel="nofollow" class="external text" href="https://www.willowgarage.com/sites/default/files/iccv09.pdf">original paper</a>).</div></div></div></div>
<p><br>
Now, the GFPFH descriptor can be computed with the result of the classification step. It will encode what the object is made of, so the robot can easily recognize it. First, an <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_2:_Cloud_processing_(basic)#Octree" title="PCL/OpenNI tutorial 2: Cloud processing (basic)"> octree</a> is created, dividing the object in voxel leaves. For every leaf, a set of probabilities is created, one for each class. Each one stores the probability of that leaf belonging to the class, and it is computed according to the number of points in that leaf that have been labelled as that class, and the total number of points. Then, for every pair of leaves in the octree, a line is casted, connecting them. Every leaf in its path is checked for occupancy, storing the result in an histogram. If the leaf is empty (free space), a value of 0 is saved. Otherwise, the leaf probabilities are used.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:502px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:GFPFH.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/GFPFH.png" width="500" height="248" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:GFPFH.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Computing the GFPFH with a voxel grid (image from <a rel="nofollow" class="external text" href="https://www.willowgarage.com/sites/default/files/iccv09.pdf">original paper</a>).</div></div></div></div>
<p><br>
The following code will compute the GFPFH for a cloud with label information. The categorization step is up to you, as it depends largely on the type of the scene, and the use you are going to give it.
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/gfpfh.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Cloud for storing the object.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZL</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZL</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the GFPFH descriptor.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">GFPFHSignature16</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">GFPFHSignature16</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Note: you should have performed preprocessing to cluster out the object</span>
	<span class="co1">// from the cloud, and save it to this individual file.</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZL</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you should now perform classification on the cloud's points. See the</span>
	<span class="co1">// original paper for more details. For this example, we will now consider 4</span>
	<span class="co1">// different classes, and randomly label each point as one of them.</span>
	<span class="kw1">for</span> <span class="br0">(</span><span class="kw4">size_t</span> i <span class="sy1">=</span> <span class="nu0">0</span><span class="sy4">;</span> i <span class="sy1">&lt;</span> object<span class="sy2">-</span><span class="sy1">&gt;</span>points.<span class="me1">size</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span> <span class="sy2">++</span>i<span class="br0">)</span>
	<span class="br0">{</span>
		object<span class="sy2">-</span><span class="sy1">&gt;</span>points<span class="br0">[</span>i<span class="br0">]</span>.<span class="me1">label</span> <span class="sy1">=</span> 1 <span class="sy2">+</span> i <span class="sy2">%</span> <span class="nu0">4</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// ESF estimation object.</span>
	pcl<span class="sy4">::</span><span class="me2">GFPFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZL</span>, pcl<span class="sy4">::</span><span class="me2">PointXYZL</span>, pcl<span class="sy4">::</span><span class="me2">GFPFHSignature16</span><span class="sy1">&gt;</span> gfpfh<span class="sy4">;</span>
	gfpfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the object that contains the labels for each point. Thanks to the</span>
	<span class="co1">// PointXYZL type, we can use the same object we store the cloud in.</span>
	gfpfh.<span class="me1">setInputLabels</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the size of the octree leaves to 1cm (cubic).</span>
	gfpfh.<span class="me1">setOctreeLeafSize</span><span class="br0">(</span>0.01<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Set the number of classes the cloud has been labelled with (default is 16).</span>
	gfpfh.<span class="me1">setNumberOfClasses</span><span class="br0">(</span>4<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	gfpfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptor<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster), Labels, Number of classes, Leaf size
</li>
<li> <b>Output</b>: GFPFH descriptor
</li>
<li> <b>Publication</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="https://www.willowgarage.com/sites/default/files/iccv09.pdf">Detecting and Segmenting Objects for Mobile Manipulation</a> (Radu Bogdan Rusu et al., 2009)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_g_f_p_f_h_estimation.html">pcl::GFPFHEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_GFPFH.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="GRSD">GRSD</span></h2>
<p>The global version of the Radius-based Surface Descriptor works in a similar fashion to GFPFH. A voxelization and a surface categorization step are performed beforehand, labelling all surface patches according to the geometric category (plane, cylinder, edge, rim, sphere), using RSD. Then, the whole cluster is classified into one of these categories, and the GRSD descriptor is computed from this.
</p><p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:402px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:GRSD.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/GRSD.png" width="400" height="241" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:GRSD.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>Classification of objects for GRSD and resulting histogram (image from <a rel="nofollow" class="external text" href="https://ias.in.tum.de/_media/spezial/bib/grsd10humanoids.pdf">original paper</a>).</div></div></div></div>
<p><br>
To compute it:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/grsd.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Object for storing the point cloud.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> cloud<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Object for storing the GRSD descriptors for each point.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">GRSDSignature21</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptors<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">GRSDSignature21</span><span class="sy1">&gt;</span><span class="br0">(</span><span class="br0">)</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>cloud<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Note: you would usually perform downsampling now. It has been omitted here</span>
	<span class="co1">// for simplicity, but be aware that computation can take a long time.</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// GRSD estimation object.</span>
	GRSDEstimation<span class="sy1">&lt;</span>PointXYZ, Normal, GRSDSignature21<span class="sy1">&gt;</span> grsd<span class="sy4">;</span>
	grsd.<span class="me1">setInputCloud</span><span class="br0">(</span>cloud<span class="br0">)</span><span class="sy4">;</span>
	grsd.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	grsd.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	<span class="co1">// Search radius, to look for neighbors. Note: the value given here has to be</span>
	<span class="co1">// larger than the radius used to estimate the normals.</span>
	grsd.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.05<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	grsd.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptors<span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p><span style="color:#606060"><i><b>NOTE: This code will only compile with PCL versions 1.8 and above (the current trunk).</b></i></span>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Points (cluster), Normals, Search method, Radius
</li>
<li> <b>Output</b>: GRSD descriptor
</li>
<li> <b>Publications</b>:
<ul>
<li> <a rel="nofollow" class="external text" href="https://ias.in.tum.de/_media/spezial/bib/grsd10humanoids.pdf">Hierarchical Object Geometric Categorization and Appearance Classification for Mobile Manipulation</a> (Zoltan-Csaba Marton et al., 2010)
</li>
<li> <a rel="nofollow" class="external text" href="https://ias.cs.tum.edu/_media/spezial/bib/marton11ijrr.pdf">Combined 2D-3D Categorization and Classification for Multimodal Perception Systems</a> (Zoltan-Csaba Marton et al., 2011)
</li>
<li> <a rel="nofollow" class="external text" href="http://www.mi.t.u-tokyo.ac.jp/top/downloadpublication/36">Voxelized Shape and Color Histograms for RGB-D</a> (Zoltan-Csaba Marton et al., 2011)
</li>
</ul>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1_g_r_s_d_estimation.html">pcl::GRSDEstimation</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_GRSD.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h1><span class="mw-headline" id="Visualization">Visualization</span></h1>
<p>Sometimes it is desired to check a visual representation of a descriptor, perhaps to analyze the distribution of data over different bins. Because they are saved as histograms, this is something trivial to do. PCL offers a couple of classes to do this.
</p>
<h2><span class="mw-headline" id="PCLHistogramVisualizer">PCLHistogramVisualizer</span></h2>
<p><span style="color:#FF1493">"PCLHistogramVisualizer"</span> is the simplest way to plot an histogram. The class has little functionality, but it does its job. Only one call is necessary to give the histogram and its size:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/vfh.h&gt;</span>
<span class="co2">#include&lt;pcl/visualization/histogram_visualizer.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Clouds for storing everything.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Estimate VFH descriptor.</span>
	pcl<span class="sy4">::</span><span class="me2">VFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span> vfh<span class="sy4">;</span>
	vfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setNormalizeBins</span><span class="br0">(</span><span class="kw2">true</span><span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setNormalizeDistance</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptor<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Plotter object.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">PCLHistogramVisualizer</span> viewer<span class="sy4">;</span>
	<span class="co1">// We need to set the size of the descriptor beforehand.</span>
	viewer.<span class="me1">addFeatureHistogram</span><span class="br0">(</span><span class="sy2">*</span>descriptor, 308<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	viewer.<span class="me1">spin</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:502px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Histogram_visualizer_VFH.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/Histogram_visualizer_VFH.png" width="500" height="189" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:Histogram_visualizer_VFH.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>VFH histogram seen with the Histogram Visualizer.</div></div></div></div>
<p><br>
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Descriptor, descriptor size, [Window size], [Background color], [Y axis range]
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1visualization_1_1_p_c_l_histogram_visualizer.html">pcl::visualization::PCLHistogramVisualizer</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_histogram_visualizer.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="PCLPlotter">PCLPlotter</span></h2>
<p>This class has all the methods from <span style="color:#FF1493">"PCLHistogramVisualizer"</span> (which will be deprecated soon) plus a lot more features. The code is almost the same:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="cpp source-cpp" style="font-family:monospace;"><pre class="de1"><span class="co2">#include &lt;pcl/io/pcd_io.h&gt;</span>
<span class="co2">#include &lt;pcl/features/normal_3d.h&gt;</span>
<span class="co2">#include &lt;pcl/features/vfh.h&gt;</span>
<span class="co2">#include&lt;pcl/visualization/pcl_plotter.h&gt;</span>
&nbsp;
<span class="kw4">int</span>
main<span class="br0">(</span><span class="kw4">int</span> argc, <span class="kw4">char</span><span class="sy2">**</span> argv<span class="br0">)</span>
<span class="br0">{</span>
	<span class="co1">// Clouds for storing everything.</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> object<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> normals<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> descriptor<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">PointCloud</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Read a PCD file from disk.</span>
	<span class="kw1">if</span> <span class="br0">(</span>pcl<span class="sy4">::</span><span class="me2">io</span><span class="sy4">::</span><span class="me2">loadPCDFile</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">(</span>argv<span class="br0">[</span>1<span class="br0">]</span>, <span class="sy2">*</span>object<span class="br0">)</span> <span class="sy3">!</span><span class="sy1">=</span> 0<span class="br0">)</span>
	<span class="br0">{</span>
		<span class="kw1">return</span> <span class="sy2">-</span><span class="nu0">1</span><span class="sy4">;</span>
	<span class="br0">}</span>
&nbsp;
	<span class="co1">// Estimate the normals.</span>
	pcl<span class="sy4">::</span><span class="me2">NormalEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span><span class="sy1">&gt;</span> normalEstimation<span class="sy4">;</span>
	normalEstimation.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setRadiusSearch</span><span class="br0">(</span>0.03<span class="br0">)</span><span class="sy4">;</span>
	pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="sy4">::</span><span class="me2">Ptr</span> kdtree<span class="br0">(</span><span class="kw3">new</span> pcl<span class="sy4">::</span><span class="me2">search</span><span class="sy4">::</span><span class="me2">KdTree</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span><span class="sy1">&gt;</span><span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	normalEstimation.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>normals<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Estimate VFH descriptor.</span>
	pcl<span class="sy4">::</span><span class="me2">VFHEstimation</span><span class="sy1">&lt;</span>pcl<span class="sy4">::</span><span class="me2">PointXYZ</span>, pcl<span class="sy4">::</span><span class="me2">Normal</span>, pcl<span class="sy4">::</span><span class="me2">VFHSignature308</span><span class="sy1">&gt;</span> vfh<span class="sy4">;</span>
	vfh.<span class="me1">setInputCloud</span><span class="br0">(</span>object<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setInputNormals</span><span class="br0">(</span>normals<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setSearchMethod</span><span class="br0">(</span>kdtree<span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setNormalizeBins</span><span class="br0">(</span><span class="kw2">true</span><span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">setNormalizeDistance</span><span class="br0">(</span><span class="kw2">false</span><span class="br0">)</span><span class="sy4">;</span>
	vfh.<span class="me1">compute</span><span class="br0">(</span><span class="sy2">*</span>descriptor<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	<span class="co1">// Plotter object.</span>
	pcl<span class="sy4">::</span><span class="me2">visualization</span><span class="sy4">::</span><span class="me2">PCLPlotter</span> plotter<span class="sy4">;</span>
	<span class="co1">// We need to set the size of the descriptor beforehand.</span>
	plotter.<span class="me1">addFeatureHistogram</span><span class="br0">(</span><span class="sy2">*</span>descriptor, 308<span class="br0">)</span><span class="sy4">;</span>
&nbsp;
	plotter.<span class="me1">plot</span><span class="br0">(</span><span class="br0">)</span><span class="sy4">;</span>
<span class="br0">}</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:502px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PCL_plotter_VFH.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PCL_plotter_VFH.png" width="500" height="187" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PCL_plotter_VFH.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>VFH histogram seen with the PCLPlotter class.</div></div></div></div>
<p><br>
If you have raw data (such as a vector of floats) you can use the <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1visualization_1_1_p_c_l_plotter.html#aaf23b2b1c2f91c517cfde387ee1b654e">addHistogramData()</a> function to plot it as an histogram.
</p>
<div style="background-color: #F8F8F8; border-style: dotted;">
<ul>
<li> <b>Input</b>: Descriptor, descriptor size, [Window size], [Background color], [Y axis range]
</li>
<li> <b>Tutorial</b>: <a rel="nofollow" class="external text" href="http://pointclouds.org/documentation/tutorials/pcl_plotter.php">PCLPlotter</a>
</li>
<li> <b>API</b>: <a rel="nofollow" class="external text" href="http://docs.pointclouds.org/trunk/classpcl_1_1visualization_1_1_p_c_l_plotter.html">pcl::visualization::PCLPlotter</a>
</li>
<li> <a rel="nofollow" class="external text" href="http://robotica.unileon.es/~victorm/PCL_plotter.tar.gz">Download</a>
</li>
</ul>
</div>
<p><br>
</p>
<h2><span class="mw-headline" id="PCL_Viewer">PCL Viewer</span></h2>
<p>This program, included with PCL, will also let you open and visualize a saved descriptor. Internally, it uses <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#PCLPlotter" title="PCL/OpenNI tutorial 4: 3D object recognition (descriptors)">PCLPlotter</a>. Remember that you can save a descriptor to a file just <a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_2:_Cloud_processing_(basic)#Writing_to_file" title="PCL/OpenNI tutorial 2: Cloud processing (basic)">like with any other cloud type</a>. Once done, invoke the viewer from the command line:
</p>
<div dir="ltr" class="mw-geshi mw-content-ltr"><div class="bash source-bash" style="font-family:monospace;"><pre class="de1">pcl_viewer <span class="sy0">&lt;</span>descriptor_file<span class="sy0">&gt;</span></pre></div></div>
<p><br>
</p>
<div class="center"><div class="thumb tnone"><div class="thumbinner" style="width:502px;"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PCL_viewer_VFH.png" class="image"><img alt="" src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PCL_viewer_VFH.png" width="500" height="188" class="thumbimage"></a>  <div class="thumbcaption"><div class="magnify"><a href="http://robotica.unileon.es/mediawiki/index.php/File:PCL_viewer_VFH.png" class="internal" title="Enlarge"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/magnify-clip.png" width="15" height="11" alt=""></a></div>VFH histogram seen with <i>pcl_viewer</i>.</div></div></div></div>
<p><br>
</p>
<hr>
<hr>
<p>Go to root: <a href="http://robotica.unileon.es/mediawiki/index.php/PhD-3D-Object-Tracking" title="PhD-3D-Object-Tracking">PhD-3D-Object-Tracking</a>
</p><p>Links to articles:
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_0:_The_very_basics" title="PCL/OpenNI tutorial 0: The very basics">PCL/OpenNI tutorial 0: The very basics</a>
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_1:_Installing_and_testing" title="PCL/OpenNI tutorial 1: Installing and testing">PCL/OpenNI tutorial 1: Installing and testing</a>
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_2:_Cloud_processing_(basic)" title="PCL/OpenNI tutorial 2: Cloud processing (basic)">PCL/OpenNI tutorial 2: Cloud processing (basic)</a>
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_3:_Cloud_processing_(advanced)" title="PCL/OpenNI tutorial 3: Cloud processing (advanced)">PCL/OpenNI tutorial 3: Cloud processing (advanced)</a>
</p><p><strong class="selflink">PCL/OpenNI tutorial 4: 3D object recognition (descriptors)</strong>
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_5:_3D_object_recognition_(pipeline)" title="PCL/OpenNI tutorial 5: 3D object recognition (pipeline)">PCL/OpenNI tutorial 5: 3D object recognition (pipeline)</a>
</p><p><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_troubleshooting" title="PCL/OpenNI troubleshooting">PCL/OpenNI troubleshooting</a>
</p>
<!-- 
NewPP limit report
CPU time usage: 0.903 seconds
Real time usage: 0.932 seconds
Preprocessor visited node count: 433/1000000
Preprocessor generated node count: 846/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 2/40
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key wikidb:pcache:idhash:334-0!*!0!!en!2!* and timestamp 20150622124858
 -->
</div>								<div class="printfooter">
				Retrieved from "<a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&oldid=4690">http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&amp;oldid=4690</a>"				</div>
												<div id="catlinks" class="catlinks catlinks-allhidden"></div>												<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
				<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul>
<li id="pt-login"><a href="http://robotica.unileon.es/mediawiki/index.php?title=Special:UserLogin&returnto=PCL%2FOpenNI+tutorial+4%3A+3D+object+recognition+%28descriptors%29" title="You are encouraged to log in; however, it is not mandatory [alt-shift-o]" accesskey="o">Log in</a></li>	</ul>
</div>
				<div id="left-navigation">
					<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul>
					<li id="ca-nstab-main" class="selected"><span><a href="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE.html" title="View the content page [alt-shift-c]" accesskey="c">Page</a></span></li>
					<li id="ca-talk" class="new"><span><a href="http://robotica.unileon.es/mediawiki/index.php?title=Talk:PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&action=edit&redlink=1" title="Discussion about the content page [alt-shift-t]" accesskey="t">Discussion</a></span></li>
			</ul>
</div>
<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
	<h3 id="mw-vector-current-variant">
		</h3>
	<h3 id="p-variants-label" tabindex="0"><span>Variants</span><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#" tabindex="-1"></a></h3>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>
				</div>
				<div id="right-navigation">
					<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul>
					<li id="ca-view" class="selected"><span><a href="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE.html">Read</a></span></li>
					<li id="ca-viewsource"><span><a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&action=edit" title="This page is protected.
You can view its source [alt-shift-e]" accesskey="e">View source</a></span></li>
					<li id="ca-history" class="collapsible"><span><a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&action=history" title="Past revisions of this page [alt-shift-h]" accesskey="h">View history</a></span></li>
			</ul>
</div>
<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
	<h3 id="p-cactions-label" tabindex="0"><span>Actions</span><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#" tabindex="-1"></a></h3>
	<div class="menu">
		<ul>
					</ul>
	</div>
</div>
<div id="p-search" role="search">
	<h3><label for="searchInput">Search</label></h3>
	<form action="http://robotica.unileon.es/mediawiki/index.php" id="searchform">
				<div id="simpleSearch">
						<input name="search" placeholder="Search" title="Search Robótica - ULE [alt-shift-f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off">						<button type="submit" name="button" title="Search the pages for this text" id="searchButton"><img src="./PCL_OpenNI tutorial 4_ 3D object recognition (descriptors) - Robótica - ULE_files/search-ltr.png" alt="Search" width="12" height="13"></button>								<input type="hidden" name="title" value="Special:Search">
		</div>
	</form>
</div>
				</div>
			</div>
			<div id="mw-panel" class="collapsible-nav">
					<div id="p-logo" role="banner"><a style="background-image: url(/mediawiki/skins/common/images/LogoRobotica_small.png);" href="http://robotica.unileon.es/mediawiki/index.php/Home" title="Visit the main page"></a></div>
				<div class="portal first persistent" role="navigation" id="p-Contents" aria-labelledby="p-Contents-label">
	<h3 id="p-Contents-label">Contents</h3>
	<div class="body">
		<ul>
			<li id="n-Home"><a href="http://robotica.unileon.es/mediawiki/index.php/Home">Home</a></li>
			<li id="n-People"><a href="http://robotica.unileon.es/mediawiki/index.php/People">People</a></li>
			<li id="n-Publications"><a href="http://robotica.unileon.es/mediawiki/index.php/Publications">Publications</a></li>
			<li id="n-Activities"><a href="http://robotica.unileon.es/mediawiki/index.php/Activities">Activities</a></li>
			<li id="n-Projects"><a href="http://robotica.unileon.es/mediawiki/index.php/Projects">Projects</a></li>
			<li id="n-Software"><a href="http://robotica.unileon.es/mediawiki/index.php/Software">Software</a></li>
		</ul>
	</div>
</div>
<div class="portal expanded" role="navigation" id="p-Wiki" aria-labelledby="p-Wiki-label">
	<h3 id="p-Wiki-label" tabindex="2"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#">Wiki</a></h3>
	<div class="body" style="display: block;">
		<ul>
			<li id="n-recentchanges"><a href="http://robotica.unileon.es/mediawiki/index.php/Special:RecentChanges" title="A list of recent changes in the wiki [alt-shift-r]" accesskey="r">Recent changes</a></li>
			<li id="n-randompage"><a href="http://robotica.unileon.es/mediawiki/index.php/Special:Random" title="Load a random page [alt-shift-x]" accesskey="x">Random page</a></li>
			<li id="n-help"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/Help:Contents" rel="nofollow" title="The place to find out">Help</a></li>
		</ul>
	</div>
</div>
<div class="portal collapsed" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
	<h3 id="p-tb-label" tabindex="3"><a href="http://robotica.unileon.es/mediawiki/index.php/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)#">Tools</a></h3>
	<div class="body">
		<ul>
			<li id="t-whatlinkshere"><a href="http://robotica.unileon.es/mediawiki/index.php/Special:WhatLinksHere/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)" title="A list of all wiki pages that link here [alt-shift-j]" accesskey="j">What links here</a></li>
			<li id="t-recentchangeslinked"><a href="http://robotica.unileon.es/mediawiki/index.php/Special:RecentChangesLinked/PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)" title="Recent changes in pages linked from this page [alt-shift-k]" accesskey="k">Related changes</a></li>
			<li id="t-specialpages"><a href="http://robotica.unileon.es/mediawiki/index.php/Special:SpecialPages" title="A list of all special pages [alt-shift-q]" accesskey="q">Special pages</a></li>
			<li id="t-print"><a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&printable=yes" rel="alternate" title="Printable version of this page [alt-shift-p]" accesskey="p">Printable version</a></li>
			<li id="t-permalink"><a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&oldid=4690" title="Permanent link to this revision of the page">Permanent link</a></li>
			<li id="t-info"><a href="http://robotica.unileon.es/mediawiki/index.php?title=PCL/OpenNI_tutorial_4:_3D_object_recognition_(descriptors)&action=info">Page information</a></li>
		</ul>
	</div>
</div>
			</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 10 April 2015, at 08:38.</li>
											<li id="footer-info-viewcount">This page has been accessed 22,683 times.</li>
									</ul>
									<div style="clear:both"></div>
		</div>
		<script>/*<![CDATA[*/window.jQuery && jQuery.ready();/*]]>*/</script><script>if(window.mw){
mw.loader.state({"site":"loading","user":"ready","user.groups":"ready"});
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.action.view.postEdit","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","skins.vector.collapsibleNav"],null,true);
}</script>
<script>if(window.mw){
mw.loader.state({"site":"ready"});
}</script>
<!-- Served in 0.088 secs. -->
	

<div class="suggestions" style="display: none; font-size: 13px;"><div class="suggestions-results"></div><div class="suggestions-special"></div></div></body></html>